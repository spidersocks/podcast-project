{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abeb2502-84eb-496f-b9db-c1ce1397ec15",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "This notebook contains the code used for topic modeling our podcast and news corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9d1a0-f176-4a55-aac9-32a971663576",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Add necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f23a7f-3b4c-48be-9ecc-688c967154cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade torch torchvision torchaudio\n",
    "#%pip install bertopic\n",
    "from bertopic import BERTopic # topic modeling\n",
    "from sentence_transformers import SentenceTransformer # embeddings\n",
    "import os # file haldling\n",
    "import pandas as pd # data exploration\n",
    "import math # for chunking function\n",
    "import pickle # for loading articles\n",
    "import random # for sampling our data\n",
    "import numpy as np # for exporting embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d46abb-129b-4010-9ffd-1c2368e280cb",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Here, we preprocess our data for topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629874db-3460-4fca-9c89-555759fad049",
   "metadata": {},
   "source": [
    "### Transcripts\n",
    "Transcripts are already clean, with no timestamps, speaker tags, or non-speech annotations. BERTopic also does not require stemming, lemmatizing, or stopword removal. \n",
    "\n",
    "Preprocessing is thus limited to removing those transcripts for podcasts published by traditional news outlets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b6722-b51b-4aa5-a385-5da56d3e71cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_df = pd.read_csv(\"data/top_50_pods_USA_2025_Q1.csv\", index_col=\"Rank\")\n",
    "\n",
    "# array of podcast titles to include\n",
    "to_include = podcast_df[~podcast_df[\"Exclude\"]][\"Title\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff162914-3ca8-42a7-a96b-39f9765e5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_table = pd.read_csv(\"data/all_podcasts_metadata.csv\")\n",
    "\n",
    "# removing all podcasts not on this list\n",
    "metadata_table = metadata_table[metadata_table[\"podcast_name\"].isin(to_include)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e1fe4-2f1b-42b0-991d-ffa718f8a8b7",
   "metadata": {},
   "source": [
    "### News Articles\n",
    "News articles do not require proprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1302e-0bc4-4f71-88ee-2cd692058781",
   "metadata": {},
   "source": [
    "## Text Chunking\n",
    "\n",
    "As podcasts and news articles can be long and often contain multiple topics, we break these into shorter documents so that our model will have an easier time grouping similar ideas together. Chunking also gives us even more datapoints, with more evidence for each topic and accordingly more robust topic discovery.\n",
    "\n",
    "For our embeddings, we default to a `chunk size` of 300, which tends to be the \"sweet spot\" (i.e. long enough to be meaningful, short enough to not mix topics too much). We can adjust this based on modeling results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ddc57a-09ba-4e2b-9667-e14c7e747871",
   "metadata": {},
   "source": [
    "### General Chunking Function\n",
    "Note, this function is designed to make chunks as even as possible without a potential tiny chunk at the end of the transcript for reasons of consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7768d0-4219-42e5-bcc1-f39a10580f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=300):\n",
    "    \"\"\"Split documents into even chunks of approximately chunk_size\"\"\"\n",
    "    words = text.split()\n",
    "    n_chunks = math.ceil(len(words) / chunk_size)\n",
    "    if n_chunks == 0:\n",
    "        return []\n",
    "    avg = len(words) // n_chunks\n",
    "    rem = len(words) % n_chunks\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for i in range(n_chunks):\n",
    "        # distribute remainder if any to the first chunk\n",
    "        extra = 1 if i < rem else 0 \n",
    "        end = start + avg + extra\n",
    "        chunks.append(' '.join(words[start:end]))\n",
    "        start = end\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7aba3-bb06-435c-8692-92f703a715d9",
   "metadata": {},
   "source": [
    "### Chunking Transcripts\n",
    "Here we apply chunking to our podcast transcripts. Our outputs are:\n",
    "\n",
    "1. `pod_docs`: a list containing chunks from all transcripts.\n",
    "2. `pod_chunk_metadata`: a list of dictionaries containing for each chunk, its source type, source name, title, publish date, filename and unique id (within the document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b00cc-86c2-471d-8da3-27e64fa932ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d136beed-89e2-4a6c-b5bc-7ba6e0a5137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_docs = []\n",
    "pod_chunk_metadata = []\n",
    "\n",
    "for idx, row in metadata_table.iterrows():\n",
    "    filepath = row[\"filename\"]\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            chunks = chunk_text(text, chunk_size=300)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                pod_docs.append(chunk)\n",
    "                # Attach all the original metadata and give each chunk a unique id for tracking\n",
    "                pod_chunk_metadata.append({\n",
    "                    \"filename\": row[\"filename\"],\n",
    "                    \"source_type\": \"podcast\",\n",
    "                    \"source_name\": row[\"podcast_name\"],\n",
    "                    \"title\": row[\"episode_title\"],\n",
    "                    \"publish_date\": row[\"publish_date\"],\n",
    "                    \"chunk_id\": i\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ea747-b0bc-45e6-be7a-06feef5037c8",
   "metadata": {},
   "source": [
    "### Chunking News Articles\n",
    "Here, we do the same chunking on our news articles. \n",
    "\n",
    "First, we load our news articles, which have been exported from Pandas DataFrames into `.pkl` files. We aggregate these into `news_articles`, a list of dictionaries where each article's text content is paired with keys for `outlet_name`, `title`, `date`, `filename`.\n",
    "\n",
    "Then, we perform the same chunking we did on our podcast transcripts. Our outputs are:\n",
    "\n",
    "1. `news_docs`: a list containing chunks from all news articles.\n",
    "2. `pod_chunk_metadata`: a list of dictionaries containing for each chunk, its source type, source name, title, publish date, filename and unique id (within the document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42d9b5-79dc-4c4b-b5e8-f9c6b80763ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_filepaths():\n",
    "    found_files = []\n",
    "    for root, _, files in os.walk(\"data/article data\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pkl\") and \"combined\" in file:\n",
    "                found_files.append(os.path.join(root, file))\n",
    "    return found_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7fe3b0-42d7-4f4f-81d2-ccb8a15954da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_news_articles(filepaths):\n",
    "    articles = []\n",
    "    for fp in filepaths:\n",
    "        outlet_name = \"NPR\" if \"NPR\" in fp else \"PBS\" if \"PBS\" in fp else \"Unknown\"\n",
    "        df = pd.read_pickle(fp)\n",
    "        for idx, row in df.iterrows():\n",
    "            if row[\"content\"]:\n",
    "                articles.append({\n",
    "                    \"outlet_name\": outlet_name,\n",
    "                    \"title\": row[\"title\"],\n",
    "                    \"date\": row.get(\"date\", None),\n",
    "                    \"content\": \" \".join(row[\"content\"]) if isinstance(row[\"content\"], list) else row[\"content\"],  # join content if it's a list\n",
    "                    \"filename\": fp\n",
    "                })\n",
    "    return articles\n",
    "\n",
    "news_filepaths = get_article_filepaths()\n",
    "news_articles = load_news_articles(news_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02147a83-3fec-499e-9a53-573708f11992",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_docs = []\n",
    "news_chunk_metadata = []\n",
    "\n",
    "for article in news_articles:\n",
    "    chunks = chunk_text(article[\"content\"], chunk_size=300)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        news_docs.append(chunk)\n",
    "        news_chunk_metadata.append({\n",
    "            \"source_type\": \"news\",\n",
    "            \"source_name\": article[\"outlet_name\"],\n",
    "            \"title\": article[\"title\"],\n",
    "            \"publish_date\": article[\"date\"],\n",
    "            \"filename\": article[\"filename\"],\n",
    "            \"chunk_id\": i\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a7207-623a-490e-a9e0-9256d4d24ead",
   "metadata": {},
   "source": [
    "## Running Topic Model\n",
    "Here we run our topic model on our combined news and podcast corpus. For the sake of time, below is a run on 10,000 randomly sampled chunks (approximately 2% of our total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bad3d-f16f-422c-aceb-067b559e8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = pod_docs + news_docs # joining our document corpora\n",
    "all_chunk_metadata = pod_chunk_metadata + news_chunk_metadata # joining our metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d07946-4c82-45e4-912f-f3e9d492b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_indices = random.sample(range(len(all_docs)), 10000) # taking a random sample of 10,000 chunks\n",
    "\n",
    "# sampled_docs = [all_docs[i] for i in sample_indices]\n",
    "# sampled_metadata = [all_chunk_metadata[i] for i in sample_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463be01b",
   "metadata": {},
   "source": [
    "# Custom Topics List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_topics = [\"Donald Trump\",\n",
    "\"Kamala Harris\",\n",
    "\"Tim Walz\",\n",
    "\"JD Vance\",\n",
    "\"Joe Biden\",\n",
    "\"Bernie Sanders\",\n",
    "\"Mike Johnson\",\n",
    "\"Nancy Pelosi\",\n",
    "\"Alexandria Ocasio-Cortez\",\n",
    "\"Mitch McConnell\",\n",
    "\"Pete Buttigieg\",\n",
    "\"Xi Jinping\",\n",
    "\"Vladimir Putin\",\n",
    "\"Volodymyr Zelensky\",\n",
    "\"Benjamin Netanyahu\",\n",
    "\"Claudia Sheinbaum\",\n",
    "\"Ali Khamenei\",\n",
    "\"Diddy\",\n",
    "\"Luigi Mangione\",\n",
    "\"Pope\",\n",
    "\"Kanye West\",\n",
    "\"Jeff Bezos\",\n",
    "\"Mark Zuckerberg\",\n",
    "\"Tim Cook\",\n",
    "\"Elon Musk\",\n",
    "\"Sam Altman\",\n",
    "\"Karoline Leavitt\",\n",
    "\"Jeffrey Epstein\",\n",
    "\"Mark Carney\",\n",
    "\"Justin Trudeau\",\n",
    "\"Robert F. Kennedy Jr\",\n",
    "\"Pete Hegseth\",\n",
    "\"Bob Menendez\",\n",
    "\"Ron DeSantis\",\n",
    "\"Kevin McCarthy\",\n",
    "\"Chuck Schumer\",\n",
    "\"Taylor Swift\", \n",
    "\"Caitlin Clark\",\n",
    "\"China\",\n",
    "\"Russia\",\n",
    "\"Canada\",\n",
    "\"United States\",\n",
    "\"Mexico\",\n",
    "\"El Salvador\",\n",
    "\"Israel\",\n",
    "\"Iran\",\n",
    "\"United Kingdom\",\n",
    "\"Taiwan\",\n",
    "\"Saudi Arabia\",\n",
    "\"India\",\n",
    "\"Pakistan\",\n",
    "\"ICE\",\n",
    "\"Democratic Party\",\n",
    "\"Republican Party\",\n",
    "\"USAID\",\n",
    "\"Texas floods\",\n",
    "\"LGBTQ\",\n",
    "\"Israel Gaza\",\n",
    "\"India Pakistan\",\n",
    "\"Russia Ukraine\",\n",
    "\"FEMA\",\n",
    "\"TikTok\",\n",
    "\"Crypto\",\n",
    "\"Tariffs\",\n",
    "\"COVID-19\",\n",
    "\"Taylor Swift Travis Kelce\",\n",
    "\"Opioids\"]\n",
    "\n",
    "print(len(seeded_topics))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7130db9-7087-4ee9-98a8-888baebfbf44",
   "metadata": {},
   "source": [
    "## Exporting Model and Topics Using Direct Classification \n",
    "We will be using our predefined topics in the seeded topics list and using the cosine similarity to determine the topic confidence of each source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing BERTopic setup\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = embedding_model.encode(all_docs, show_progress_bar=True)\n",
    "topic_model = BERTopic(embedding_model=embedding_model, verbose=True, seed_topic_list=seeded_topics)\n",
    "topics, probs = topic_model.fit_transform(all_docs, embeddings)\n",
    "\n",
    "# embeddings for direct classification\n",
    "print(\"Encoding seeded topics...\")\n",
    "topic_embeddings = embedding_model.encode(seeded_topics, show_progress_bar=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab1d6f3",
   "metadata": {},
   "source": [
    "## Direct Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_topics = []\n",
    "similarity_scores = []\n",
    "\n",
    "print(f\"Using existing embeddings for {len(embeddings)} documents...\")\n",
    "print(f\"Expected to process {len(all_docs)} documents...\")\n",
    "\n",
    "\n",
    "# if len(embeddings) != len(all_docs):\n",
    "#     print(f\"WARNING: Embeddings length ({len(embeddings)}) doesn't match documents length ({len(all_docs)})\")\n",
    "#     min_length = min(len(embeddings), len(all_docs))\n",
    "#     print(f\"Processing only first {min_length} documents\")\n",
    "#     process_count = min_length\n",
    "# else:\n",
    "#     process_count = len(embeddings)\n",
    "\n",
    "process_count = len(embeddings)\n",
    "for i in range(process_count):\n",
    "    doc_emb = embeddings[i]\n",
    "    \n",
    "\n",
    "    doc_emb_norm = doc_emb / np.linalg.norm(doc_emb)\n",
    "    topic_embs_norm = topic_embeddings / np.linalg.norm(topic_embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    similarities = np.dot(topic_embs_norm, doc_emb_norm)\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    \n",
    "    custom_topics.append(seeded_topics[most_similar_idx])\n",
    "    similarity_scores.append(similarities[most_similar_idx])\n",
    "    \n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"Processed {i + 1}/{process_count} documents\")\n",
    "\n",
    "print(f\"Direct classification complete. Processed {len(custom_topics)} documents.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7eb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_bertopic_probs(probs):\n",
    "    \"\"\"Handle different probability formats from BERTopic\"\"\"\n",
    "    if probs is None:\n",
    "        return [0.0] * len(topics)\n",
    "    \n",
    "    if len(probs.shape) == 1:\n",
    "        return probs.tolist()\n",
    "    elif len(probs.shape) == 2:\n",
    "        return probs.max(axis=1).tolist()\n",
    "    else:\n",
    "        print(f\"Warning: using zeros\")\n",
    "        return [0.0] * len(topics)\n",
    "\n",
    "bertopic_probs = get_bertopic_probs(probs)\n",
    "\n",
    "# print(f\"\\nDebugging array lengths:\")\n",
    "# print(f\"  ]=: {len(all_docs)}\")\n",
    "# print(f\"  topics: {len(topics)}\")\n",
    "# print(f\"  bertopic_probs: {len(bertopic_probs)}\")\n",
    "# print(f\"  custom_topics: {len(custom_topics)}\")\n",
    "# print(f\"  similarity_scores: {len(similarity_scores)}\")\n",
    "# print(f\"  embeddings: {len(embeddings)}\")\n",
    "\n",
    "expected_length = len(all_docs)\n",
    "arrays_to_check = {\n",
    "    'topics': topics,\n",
    "    'bertopic_probs': bertopic_probs,\n",
    "    'custom_topics': custom_topics,\n",
    "    'similarity_scores': similarity_scores\n",
    "}\n",
    "\n",
    "for name, array in arrays_to_check.items():\n",
    "    if len(array) != expected_length:\n",
    "        print(f\"ERROR: {name} has length {len(array)}, expected {expected_length}\")\n",
    "        if len(array) > expected_length:\n",
    "            print(f\"  Truncating {name} to {expected_length}\")\n",
    "            arrays_to_check[name] = array[:expected_length]\n",
    "        else:\n",
    "            print(f\"  Padding {name} to {expected_length}\")\n",
    "            if name == 'topics':\n",
    "                arrays_to_check[name] = list(array) + [-1] * (expected_length - len(array))\n",
    "            elif name == 'bertopic_probs':\n",
    "                arrays_to_check[name] = list(array) + [0.0] * (expected_length - len(array))\n",
    "            elif name == 'custom_topics':\n",
    "                arrays_to_check[name] = list(array) + ['Unknown'] * (expected_length - len(array))\n",
    "            elif name == 'similarity_scores':\n",
    "                arrays_to_check[name] = list(array) + [0.0] * (expected_length - len(array))\n",
    "\n",
    "topics = arrays_to_check['topics']\n",
    "bertopic_probs = arrays_to_check['bertopic_probs']\n",
    "custom_topics = arrays_to_check['custom_topics']\n",
    "similarity_scores = arrays_to_check['similarity_scores']\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'text': all_docs,\n",
    "    'bertopic_topic': topics,\n",
    "    'bertopic_prob': bertopic_probs,\n",
    "    'custom_topic': custom_topics,\n",
    "    'custom_topic_score': similarity_scores\n",
    "})\n",
    "\n",
    "\n",
    "results_df['custom_topic_confident'] = results_df['custom_topic_score'] > 0.3  #change this cosine similarity score threshold as desired\n",
    "\n",
    "\n",
    "custom_counts = pd.Series(custom_topics).value_counts()\n",
    "print(f\"\\nDirect classification - Top 10 topics:\")\n",
    "for topic, count in custom_counts.head(10).items():\n",
    "    avg_score = results_df[results_df['custom_topic'] == topic]['custom_topic_score'].mean()\n",
    "    print(f\"  {topic}: {count} documents (avg score: {avg_score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results_df))\n",
    "\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b584d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('custom_topics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
