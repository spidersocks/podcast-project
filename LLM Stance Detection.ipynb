{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924f5b33-5a2d-4814-b081-73f4b3c6f9fe",
   "metadata": {},
   "source": [
    "# LLM Stance Detection\n",
    "This notebook contains code for the first attempt at LLM Chain of Stance (COS) stance labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4272c6ec-99e3-4bf7-9fb1-894118c75f47",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f80c4-aef4-4d49-9bcf-9cae1a33544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install/update relevant packages\n",
    "# !pip install --upgrade transformers accelerate torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4161a-887d-4a34-92e7-c344a84f0a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # setting up our LLM\n",
    "import torch # setting up our LLM\n",
    "from huggingface_hub import login # authentication\n",
    "import re # stance extraction\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import gc\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce729ff3-9126-44e7-81f3-e62e7cfd20c2",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e251e-bcc2-4a7a-a0f4-0ed07023dd6d",
   "metadata": {},
   "source": [
    "### Choice of LLM\n",
    "The model used is [Ministral-8B-Instruct-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410):\n",
    "1. Mistral's 7-B model outperformed similarly sized Qwen1.5 and LLaMA3 models in leading [Chain of Stance research](https://arxiv.org/pdf/2408.04649).\n",
    "2. It is Mistral's newest and most powerful model under 10B parameters.\n",
    "3. It supports a 128k context window, better for long-form and recent data.\n",
    "   \n",
    "**Note**: You must agree to the Mistral AI Research License for access and use it for research/non-commercial purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f759d2-7a37-4e57-82d4-83f64891296e",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "1. Sign up for a Hugging Face account: [https://huggingface.co/join](https://huggingface.co/join)\n",
    "2. Request access to Ministral-8B-Instruct-2410: https://huggingface.co/mistralai/Ministral-8B-Instruct-2410\n",
    "3. Insert your own access token below (replace \"hf_xxx\" with your actual token):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560fcf5-5bef-4c5b-86b0-af6264e1ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authentication to use model\n",
    "ACCESS_TOKEN = \"hf____\"\n",
    "\n",
    "login(ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc0377-1c93-4233-a67e-b29a03606c8b",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66891d-3ba3-4138-aa20-69be96c1fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "\n",
    "# Load tokenizer and model (uses GPU if available)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478c2c6-9da5-42f0-859d-37edfb114f5d",
   "metadata": {},
   "source": [
    "### Creating Pipeline\n",
    "Chaining our model and tokenizer together in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b70cfd-8970-4482-985b-fdb292e4e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,  # making output deterministic for reproducibility\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05071cf-c568-4110-afd0-159365602a52",
   "metadata": {},
   "source": [
    "## Chain of Stance Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739ff17-76af-4e43-b77b-94fd5ad8ec7b",
   "metadata": {},
   "source": [
    "### Prompting\n",
    "Function `create_cos_prompt` to generate our Chain of Stance prompt.\n",
    "\n",
    "Per the Chain of Stance framework, the prompting encourages the model to sequentially consider multiple aspects of the text before making a stance judgement.\n",
    "\n",
    "Inputs to the function are `text`, `topic`, `source_name` and `title.\n",
    "\n",
    "We optionally allow `example` to be input. This should be a dictionary containing an example text snippet, along with the desired LLM response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf72309-40af-4dbf-9f78-3c334942b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cos_prompt(text, topic, source_name, title, example=None):\n",
    "    # this prompt structure guides the LLM through the 6 steps of CoS.\n",
    "    # the stances are FAVOR, AGAINST, NONE.\n",
    "    # an example can also be provided for few-shot prompting\n",
    "    # keys must be 'text', 'topic', 'source_name', 'title' for a given example, and 'answer', the ideal LLM response\n",
    "    \n",
    "    main_prompt = f\"\"\"You are an expert in stance detection. \n",
    "                 Your task is to determine the stance of a given text towards a specific topic. \n",
    "                 Follow these steps carefully to provide a complete analysis and a final conclusion.\n",
    "\n",
    "                **Source Name:** \"{source_name}\"\n",
    "                **Title:** \"{title}\"\n",
    "                **Text for Analysis:** \"{text}\"\n",
    "                **Topic:** \"{topic}\"\n",
    "                \n",
    "                **Step 1: Contextual Information Analysis**\n",
    "                Analyze the contextual information of the text. \n",
    "                Consider the topic, the likely identity of the author, the target audience, and any relevant socio-cultural background.\n",
    "                \n",
    "                **Step 2: Main Idea and Viewpoint Identification**\n",
    "                Based on the text and context, what are the core viewpoints and main intentions being expressed regarding the topic?\n",
    "                \n",
    "                **Step 3: Language and Emotional Attitude Analysis**\n",
    "                Analyze the language, tone, and emotion. \n",
    "                Identify emotive words, rhetorical devices, and the author's overall tone (e.g., affirmative, negative, neutral, sarcastic).\n",
    "                \n",
    "                **Step 4: Comparison with Possible Stances**\n",
    "                Compare the text's content and tone against the three possible stances (FAVOR, AGAINST, NONE). \n",
    "                For each stance, list evidence from the source (if any) of that stance.\n",
    "                \n",
    "                **Step 5: Logical Inference and Consistency Check**\n",
    "                Synthesize your analysis from all previous steps to make a final decision on the most likely stance expressed in the text from (FAVOR, AGAINST, NONE).\n",
    "                \n",
    "                **Step 6: Final Stance Determination**\n",
    "                 Output the final stance on a new line, in the format 'Final Stance: [STANCE]', where [STANCE] is one of FAVOR, AGAINST, or NONE.\n",
    "                \n",
    "                Begin your analysis now.\n",
    "                \"\"\"\n",
    "\n",
    "    # If no example, this is the prompt\n",
    "    if example is None:\n",
    "        return \"[TASK]\\n\" + main_prompt.strip() + \"\\n[/TASK]\"\n",
    "\n",
    "    # Otherwise, format in the example\n",
    "    example_prompt = f\"\"\"You are an expert in stance detection. \n",
    "                 Your task is to determine the stance of a given text towards a specific topic. \n",
    "                 Follow these steps carefully to provide a complete analysis and a final conclusion.\n",
    "\n",
    "                **Source Name:** \"{example[\"source_name\"]}\"\n",
    "                **Title:** \"{example[\"title\"]}\"\n",
    "                **Text for Analysis:** \"{example[\"text\"]}\"\n",
    "                **Topic:** \"{example[\"topic\"]}\"\n",
    "                \n",
    "                **Step 1: Contextual Information Analysis**\n",
    "                Analyze the contextual information of the text. \n",
    "                Consider the topic, the likely identity of the author, the target audience, and any relevant socio-cultural background.\n",
    "                \n",
    "                **Step 2: Main Idea and Viewpoint Identification**\n",
    "                Based on the text and context, what are the core viewpoints and main intentions being expressed regarding the topic?\n",
    "                \n",
    "                **Step 3: Language and Emotional Attitude Analysis**\n",
    "                Analyze the language, tone, and emotion. \n",
    "                Identify emotive words, rhetorical devices, and the author's overall tone (e.g., affirmative, negative, neutral, sarcastic).\n",
    "                \n",
    "                **Step 4: Comparison with Possible Stances**\n",
    "                Compare the text's content and tone against the three possible stances (FAVOR, AGAINST, NONE). \n",
    "                For each stance, list evidence from the source (if any) of that stance.\n",
    "                \n",
    "                **Step 5: Logical Inference and Consistency Check**\n",
    "                Synthesize your analysis from all previous steps to make a final decision on the most likely stance expressed in the text from (FAVOR, AGAINST, NONE).\n",
    "                \n",
    "                **Step 6: Final Stance Determination**\n",
    "                 Output the final stance on a new line, in the format 'Final Stance: [STANCE]', where [STANCE] is one of FAVOR, AGAINST, or NONE.\n",
    "                \n",
    "                Begin your analysis now.\n",
    "                \"\"\"\n",
    "\n",
    "    # Compose final multi-shot prompt\n",
    "    prompt = (\n",
    "        \"[EXAMPLE]\\n\"\n",
    "        + \"**Example Prompt**\\n\"\n",
    "        + example_prompt.strip()\n",
    "        + \"\\n**Model Response**\\n\"\n",
    "        + example[\"answer\"].strip()\n",
    "        + \"\\n[/EXAMPLE]\"\n",
    "        + \"\\n\\n[TASK]\\n\"\n",
    "        + main_prompt.strip()\n",
    "        + \"\\n[/TASK]\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9c78f-2256-40e4-a053-8cbc9ddcc411",
   "metadata": {},
   "source": [
    "### Few-Shot Prompting Example\n",
    "This contains sample text paired with optimal model output, all sourced from Figure 2 of [Chain of Stance: Stance Detection with Large Language Models](https://arxiv.org/pdf/2408.04649) (Ma et al.). This can optionally be provided to the model for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad9af0-447e-44cb-8fb8-67cf2b3390cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text\n",
    "example_text = \"I love the smell of Hillary in the morning. It smells like Republican Victory. #TedCruz2016 #Marco Rubio\"\n",
    "example_topic = \"Hillary Clinton\"\n",
    "example_source = \"Twitter\"\n",
    "example_title = \"Hillary Clinton Tweet\"\n",
    "\n",
    "# sample response\n",
    "example_answer = \"\"\"\n",
    "**Recognizing Context**\n",
    "Text Topic: The tweet mentions Hillary Clinton and her political competitors. Specifically, it references Ted Cruz and Marco Rubio, and includes the hashtag #SemST.\n",
    "\n",
    "Author's Identity: The specific identity of the author is not directly indicated, but it can be inferred that the author is likely a political commentator or an ordinary user with certain political inclinations.\n",
    "\n",
    "Target Audience: The target audience of the tweet is likely those who follow American politics, especially the internal competition within the Republican Party.\n",
    "\n",
    "Relevant Socio-Cultural Background: The tweet was posted during the U.S. presidential election period, where Hillary Clinton was a major candidate for the Democratic Party, while Ted Cruz and Marco Rubio were major candidates for the Republican Party.\n",
    "\n",
    "**Analyzing Main Idea**\n",
    "Core viewpoint: The text expresses negative feelings towards Hillary Clinton and is optimistic about a Republican victory. \n",
    "\n",
    "Main Intention: To express support for the Republican candidates (like Ted Cruz and Marco Rubio) by mocking Hillary Clinton.\n",
    "\n",
    "**Emotional Analysis**\n",
    "Language Expression: \"I love the smell of Hillary in the morning. It smells like Republican Victory.\" The phrase uses sarcasm. The word \"love\" is usually positive, but here it is clearly sarcastic.\n",
    "\n",
    "Rhetorical Strategy: The author adapts a famous line from the movie \"Apocalypse Now\" (\"I love the smell of napalm in the morning\"), changing it to \"I love the smell of Hillary in the morning\" to mock Hillary Clinton and suggest that her presence will lead to a Republican victory. \n",
    "\n",
    "Tone: The overall tone is clearly sarcastic and derogatory.\n",
    "\n",
    "Emotion: The underlying emotional inclination in the tweet is dissatisfaction with Hillary Clinton and a hope for a Republican victory.\n",
    "\n",
    "**Stance Reinforcement**\n",
    "Comparing neutral, favor, and against stances:\n",
    "Favor Hillary Clinton: There are no signs of support for her in the tweet.\n",
    "Against Hillary Clinton: The tweet mocks and belittles her, indicating a clear opposition to her.\n",
    "None: The tweet expresses clear emotions and cannot be neutral.\n",
    "\n",
    "**Logical Inference**\n",
    "Combining the contextual information, main idea, emotional attitude and stance reinforcement, the tweet is clearly belittling Hillary Clinton and expressing support for Republican candidates like Ted Cruz and Marco Rubio.\n",
    "\n",
    "**Stance Determination**\n",
    "Final Stance: [AGAINST]\n",
    "\"\"\".strip()\n",
    "\n",
    "example_dict = {\"source_name\":example_source, \"title\":example_title, \"text\":example_text, \"topic\":example_topic, \"answer\":example_answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41044f45-21b2-424f-8c2c-12db4ac5d7a8",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Function `get_stance_with_cos` combined our CoS prompt with our chatbot pipeline to generate a stance on a given document.\n",
    "\n",
    "Inputs are the text, topic, source_name, title, example (optional), and model pipeline.\n",
    "\n",
    "Outputs are two strings:\n",
    "\n",
    "1. The `stance` (one of \"FAVOR\", \"AGAINST\", or \"NONE\")\n",
    "2. The full six step `reasoning process` for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1233db1-3c93-4785-96a6-c72047b57cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stance_with_cos(text, topic, source_name, title, chatbot_pipeline, example=None):\n",
    "    \"\"\"\n",
    "    Uses CoS method to get a stance from the LLM, with optional few-shot example.\n",
    "    \"\"\"\n",
    "    # create the full prompt using CoS template\n",
    "    cos_prompt = create_cos_prompt(text, topic, source_name, title, example=example)\n",
    "    \n",
    "    # tag the prompt\n",
    "    formatted_prompt = f\"[INST]{cos_prompt}[/INST]\"\n",
    "    \n",
    "    response = chatbot_pipeline(formatted_prompt)[0][\"generated_text\"]\n",
    "    \n",
    "    reasoning_text = response.split(\"[/INST]\")[-1].strip()\n",
    "    \n",
    "    match = re.search(r\"Final Stance:\\s*(FAVOR|AGAINST|NONE)\", reasoning_text, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        final_stance = match.group(1).upper()\n",
    "    else:\n",
    "        final_stance = None\n",
    "        \n",
    "    return final_stance, reasoning_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e784b0c-9a79-41a9-b178-f13023ad25b3",
   "metadata": {},
   "source": [
    "### Test Run\n",
    "Single test run with sample data to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664fbfe4-da0e-4285-9c33-c5759fb903e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"@realDonaldTrump You are not fooling anyone. You're scared, and overwhelmed, and you have absolutely no idea what you're doing. And it shows\"\n",
    "# topic = \"Donald Trump\"\n",
    "# source_name = \"NBC News\"\n",
    "# title = \"Woman's Epic Anti-Trump Twitter Rant Goes Viral\"\n",
    "\n",
    "# stance, reasoning = get_stance_with_cos(text, topic, source_name, title, chatbot, example_dict)\n",
    "\n",
    "# print(\"--- STANCE ---\")\n",
    "# print(stance)\n",
    "# print(\"\\n--- REASONING ---\")\n",
    "# print(reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24931c-a1c8-4d50-9b86-04d68ee5b0cd",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Loading our chunk data with topic labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59416c-065f-4e03-b21e-789976ccf6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_w_topic_labels = pd.read_csv(\"data/chunks_for_stance_detection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1663cd-c297-4f2a-888c-363e5157ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(chunks_w_topic_labels.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec3f58-74ef-4f9e-ae03-93475856030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cols = [col for col in chunks_w_topic_labels.columns if col.startswith(\"topic_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f07ffc-77d9-4e14-8bc8-8e1293025eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_cols = {f\"stance_{col.replace(\"topic_\", \"\")}\": None for col in topic_cols}\n",
    "reasoning_cols = {f\"reasoning_{col.replace(\"topic_\", \"\")}\": None for col in topic_cols}\n",
    "\n",
    "new_cols_df = pd.DataFrame([stance_cols | reasoning_cols], index=chunks_w_topic_labels.index)\n",
    "chunks_w_topic_labels = pd.concat([chunks_w_topic_labels, new_cols_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ad7ad-7af6-41e9-bd1a-7374a6a4bd8c",
   "metadata": {},
   "source": [
    "## Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136e6ff-890c-480d-ad17-e872d9a5ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_stance_detection_wide(df, chatbot_pipeline, batch_size=8, example_dict=None, output_path=\"results_batches/\"):\n",
    "    import os\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    rows = df.to_dict(\"records\")\n",
    "    n = len(rows)\n",
    "    # Track processed row indices for checkpointing\n",
    "    processed_batches = []\n",
    "    for batch_num, batch_start in enumerate(tqdm(range(0, n, batch_size))):\n",
    "        batch_rows = rows[batch_start:batch_start+batch_size]\n",
    "        prompts = []\n",
    "        meta = []\n",
    "        for row_idx, row in enumerate(batch_rows, start=batch_start):\n",
    "            for col in topic_cols:\n",
    "                if row[col]:\n",
    "                    topic_name = col.replace(\"topic_\", \"\").replace(\"_\", \" \")\n",
    "                    prompt = create_cos_prompt(\n",
    "                        row[\"text\"],\n",
    "                        topic_name,\n",
    "                        row[\"source_name\"],\n",
    "                        row[\"title\"],\n",
    "                        example=example_dict\n",
    "                    )\n",
    "                    prompts.append(f\"[INST]{prompt}[/INST]\")\n",
    "                    meta.append((row_idx, topic_name))\n",
    "        if len(prompts) == 0:\n",
    "            continue\n",
    "        try:\n",
    "            results = chatbot_pipeline(prompts)\n",
    "        except Exception as e:\n",
    "            print(f\"Batch {batch_num} failed: {e}\")\n",
    "            continue\n",
    "        batch_updates = []\n",
    "        for (row_idx, topic_name), output in zip(meta, results):\n",
    "            reasoning_text = output[\"generated_text\"].split(\"[/INST]\")[-1].strip()\n",
    "            match = re.search(r\"Final Stance:\\\\s*(FAVOR|AGAINST|NONE)\", reasoning_text, re.IGNORECASE)\n",
    "            stance = match.group(1).upper() if match else None\n",
    "            batch_updates.append({\n",
    "                \"row_idx\": row_idx,\n",
    "                \"topic_name\": topic_name,\n",
    "                f\"stance_{topic_name}\": stance,\n",
    "                f\"reasoning_{topic_name}\": reasoning_text\n",
    "            })\n",
    "        # Convert to DataFrame and save as parquet\n",
    "        batch_df = pd.DataFrame(batch_updates)\n",
    "        batch_df.to_parquet(os.path.join(output_path, f\"batch_{batch_num:05d}.parquet\"))\n",
    "        processed_batches.append(batch_num)\n",
    "        # Optionally, free up memory\n",
    "        del batch_df, batch_updates, results\n",
    "        gc.collect()\n",
    "    # Optionally, return the list of processed batch files\n",
    "    return processed_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071aeda2-81af-4421-b6df-6dc480fb5ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # or 16 if fits in VRAM\n",
    "\n",
    "# Run the batch stance detection with incremental writing\n",
    "processed_batches = batch_stance_detection_wide(\n",
    "    chunks_w_topic_labels,\n",
    "    chatbot_pipeline=chatbot,\n",
    "    batch_size=batch_size,\n",
    "    example_dict=example_dict,\n",
    "    output_path=\"results_batches/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03653360-63d4-4350-996d-5d27eafb0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all batch parquet files and concatenate\n",
    "batch_files = sorted(glob.glob(\"results_batches/batch_*.parquet\"))\n",
    "all_batches = [pd.read_parquet(f) for f in batch_files]\n",
    "final_results = pd.concat(all_batches, ignore_index=True)\n",
    "\n",
    "# Optionally, merge results back to your original DataFrame on row index\n",
    "for idx, row in final_results.iterrows():\n",
    "    r_idx = row[\"row_idx\"]\n",
    "    tname = row[\"topic_name\"]\n",
    "    chunks_w_topic_labels.at[r_idx, f\"stance_{tname}\"] = row[f\"stance_{tname}\"]\n",
    "    chunks_w_topic_labels.at[r_idx, f\"reasoning_{tname}\"] = row[f\"reasoning_{tname}\"]\n",
    "\n",
    "# Save the final DataFrame\n",
    "chunks_w_topic_labels.to_parquet(\"chunks_with_stances_final.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
