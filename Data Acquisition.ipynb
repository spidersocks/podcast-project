{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b9ca9a-4600-4a68-8e3d-294b62384ed3",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "This notebook contains the code used to download 1 year's worth of transcripts from the [top 50 most listened to podcasts](https://www.edisonresearch.com/the-top-50-podcasts-in-the-u-s-for-q1-2025-from-edison-podcast-metrics/) in the US for the first quarter of 2025.\n",
    "\n",
    "Transcripts are hosted on [podscribe.app](https://podscribe.app/), and have all been transcribed using Google Cloud's [Google Cloud's Speech-to-Text API](https://cloud.google.com/speech-to-text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368fa5a8-25b2-479e-8efa-65130ac9278f",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Add necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c57e9-687b-494a-b793-3a150c6b9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from playwright.async_api import async_playwright\n",
    "from playwright.async_api import TimeoutError as PlaywrightTimeoutError\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49430f-f43e-482d-82f5-daf864cc0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automated downloading of transcripts from Podscribe\n",
    "# does this by navigating through relevant episode pages, toggling UI controls, and saving transcripts\n",
    "\n",
    "# sanitizing strings for safe filenames\n",
    "def sanitize_filename(s):\n",
    "    return re.sub(r\"[^a-zA-Z0-9_\\- \\(\\)\\[\\]\\.,]\", \"\", s).strip()\n",
    "\n",
    "# Get the episode title from the page header\n",
    "async def extract_episode_title(page):\n",
    "    selectors = [\n",
    "        \"h1\", \"h2\", \".MuiTypography-h1\", \".MuiTypography-h2\"\n",
    "    ]\n",
    "    for sel in selectors:\n",
    "        el = await page.query_selector(sel)\n",
    "        if el:\n",
    "            text = (await el.inner_text()).strip()\n",
    "            if text and len(text) > 3:\n",
    "                match = re.split(r\"\\s+episode\\b\", text, flags=re.I)\n",
    "                return match[0].strip()\n",
    "    # fallback: use the <title> tag\n",
    "    title_el = await page.query_selector(\"\"\"title\"\"\")\n",
    "    if title_el:\n",
    "        text = (await title_el.inner_text()).strip()\n",
    "        match = re.split(r\"\\s+episode\\b\", text, flags=re.I)\n",
    "        return match[0].strip()\n",
    "    return \"Untitled\"\n",
    "\n",
    "# pause briefly to mimic human browsing, with mouse and scroll movements\n",
    "async def random_human_sleep(page, min_ms=800, max_ms=3500, move_mouse=True, scroll=True):\n",
    "    t = random.uniform(min_ms, max_ms)\n",
    "    print(f\"Sleeping for {t:.2f} ms...\")\n",
    "    if move_mouse:\n",
    "        for _ in range(random.randint(1, 3)):\n",
    "            x = random.randint(0, 1200)\n",
    "            y = random.randint(0, 800)\n",
    "            await page.mouse.move(x, y, steps=random.randint(5, 15))\n",
    "            await asyncio.sleep(random.uniform(0.08, 0.25))\n",
    "    if scroll and random.random() < 0.5:\n",
    "        pixels = random.randint(-400, 400)\n",
    "        await page.mouse.wheel(0, pixels)\n",
    "    await page.wait_for_timeout(t)\n",
    "\n",
    "# toggling off switches to remove timestamps and speaker tags\n",
    "async def toggle_switch(page, label_text):\n",
    "    labels = await page.query_selector_all(\"\"\"label.MuiFormControlLabel-root\"\"\")\n",
    "    found = False\n",
    "    for label in labels:\n",
    "        spans = await label.query_selector_all(\"\"\"span\"\"\")\n",
    "        if not spans:\n",
    "            continue\n",
    "        text = await spans[-1].inner_text()\n",
    "        if text.strip().lower() == label_text.lower():\n",
    "            found = True\n",
    "            checkbox = await label.query_selector(\"\"\"input[type=\"checkbox\"]\"\"\")\n",
    "            if checkbox:\n",
    "                checked = await checkbox.is_checked()\n",
    "                if checked:\n",
    "                    box = await label.bounding_box()\n",
    "                    if box:\n",
    "                        x = box[\"x\"] + box[\"width\"] / 2\n",
    "                        y = box[\"y\"] + box[\"height\"] / 2\n",
    "                        print(f\"Moving mouse to switch '{label_text}' at ({x:.1f}, {y:.1f})\")\n",
    "                        await page.mouse.move(x, y, steps=random.randint(5, 10))\n",
    "                        await asyncio.sleep(random.uniform(0.08, 0.15))\n",
    "                    print(f\"Toggling off '{label_text}'\")\n",
    "                    await checkbox.click(force=True)\n",
    "                else:\n",
    "                    print(f\"'{label_text}' already off\")\n",
    "            else:\n",
    "                print(f\"Checkbox for '{label_text}' not found\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"Label for '{label_text}' not found\")\n",
    "    return found\n",
    "\n",
    "# Retry toggling with timeout handling if fails\n",
    "async def safe_toggle_switch(page, label_text, max_retries=2):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = await toggle_switch(page, label_text)\n",
    "            return result\n",
    "        except PlaywrightTimeoutError as e:\n",
    "            print(f\"Timeout clicking '{label_text}' switch, attempt {attempt+1}/{max_retries}: {e}\")\n",
    "            await random_human_sleep(page, 1500, 3000)\n",
    "    print(f\"Giving up on toggling '{label_text}'. Continuing...\")\n",
    "    return False\n",
    "\n",
    "# puts everything together to download Podscribe transcript for a given episode\n",
    "async def download_podscribe_transcript(page, transcript_url: str, save_dir: str, published_date_str: str):\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Navigating to {transcript_url}\")\n",
    "    try:\n",
    "        await page.goto(transcript_url, timeout=60000)\n",
    "    except PlaywrightTimeoutError as e:\n",
    "        print(f\"Timeout navigating to {transcript_url}: {e}\")\n",
    "        return\n",
    "\n",
    "    await random_human_sleep(page)\n",
    "\n",
    "    episode_title = await extract_episode_title(page)\n",
    "    episode_title = sanitize_filename(episode_title)\n",
    "    published_date_str = sanitize_filename(published_date_str)\n",
    "    base_filename = f\"{episode_title} ({published_date_str}).txt\"\n",
    "\n",
    "    await random_human_sleep(page)\n",
    "    await safe_toggle_switch(page, \"\"\"Speakers\"\"\")\n",
    "    await random_human_sleep(page)\n",
    "    await safe_toggle_switch(page, \"\"\"Times\"\"\")\n",
    "    await random_human_sleep(page)\n",
    "\n",
    "    try:\n",
    "        download_btn = await page.query_selector(\"\"\"button svg[data-testid=\"GetAppIcon\"]\"\"\")\n",
    "        if download_btn:\n",
    "            parent_btn = await download_btn.evaluate_handle(\"\"\"el => el.closest(\"button\")\"\"\")\n",
    "            box = await parent_btn.bounding_box()\n",
    "            if box:\n",
    "                x = box[\"x\"] + box[\"width\"] / 2\n",
    "                y = box[\"y\"] + box[\"height\"] / 2\n",
    "                print(f\"Moving mouse to Download button at ({x:.1f}, {y:.1f})\")\n",
    "                await page.mouse.move(x, y, steps=random.randint(5, 10))\n",
    "                await asyncio.sleep(random.uniform(0.08, 0.18))\n",
    "            print(\"Clicking download button...\")\n",
    "            await parent_btn.click()\n",
    "            await random_human_sleep(page, 800, 1600)\n",
    "            txt_option = await page.wait_for_selector(\"\"\"li span.jss97:text(\"TXT\")\"\"\", timeout=5000)\n",
    "            if txt_option:\n",
    "                txt_box = await txt_option.bounding_box()\n",
    "                if txt_box:\n",
    "                    x = txt_box[\"x\"] + txt_box[\"width\"] / 2\n",
    "                    y = txt_box[\"y\"] + txt_box[\"height\"] / 2\n",
    "                    print(f\"Moving mouse to TXT dropdown at ({x:.1f}, {y:.1f})\")\n",
    "                    await page.mouse.move(x, y, steps=random.randint(5, 10))\n",
    "                    await asyncio.sleep(random.uniform(0.08, 0.15))\n",
    "                async with page.expect_download() as download_info:\n",
    "                    await txt_option.click()\n",
    "                download = await download_info.value\n",
    "                save_path = os.path.join(save_dir, base_filename)\n",
    "                await download.save_as(save_path)\n",
    "                print(f\"Transcript downloaded to {save_path}\")\n",
    "            else:\n",
    "                print(\"TXT option not found!\")\n",
    "        else:\n",
    "            print(\"Download button not found.\")\n",
    "    except PlaywrightTimeoutError as e:\n",
    "        print(f\"Timeout during download for {transcript_url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading transcript for {transcript_url}: {e}\")\n",
    "\n",
    "    await random_human_sleep(page)\n",
    "\n",
    "# get all available transcripts on a given page\n",
    "async def scrape_and_download(series_url, save_dir, months=12, old_limit=10):\n",
    "    cutoff = datetime.now() - timedelta(days=30*months)\n",
    "    page_num = 1\n",
    "\n",
    "    from playwright.async_api import async_playwright\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)\n",
    "        context = await browser.new_context(accept_downloads=True)\n",
    "        page = await context.new_page()\n",
    "        await page.set_extra_http_headers({\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                          \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                          \"Chrome/124.0.0.0 Safari/537.36\"\n",
    "        })\n",
    "        await page.goto(series_url)\n",
    "        await page.wait_for_timeout(2000)\n",
    "\n",
    "        while True:\n",
    "            # scroll to bottom to make sure all episodes load\n",
    "            for _ in range(10):\n",
    "                await page.mouse.wheel(0, 1000)\n",
    "                await page.wait_for_timeout(200)\n",
    "            await page.wait_for_timeout(1000)\n",
    "\n",
    "            rows = await page.query_selector_all(\"\"\"tr\"\"\")\n",
    "            print(f\"Page {page_num}: found {len(rows)} table rows.\")\n",
    "            old_count = 0\n",
    "            page_episodes = []\n",
    "\n",
    "            # get all episode links and published dates\n",
    "            for row in rows:\n",
    "                anchor = await row.query_selector(\"\"\"a[href^=\"/episode/\"]\"\"\")\n",
    "                if not anchor:\n",
    "                    continue\n",
    "                href = await anchor.get_attribute(\"href\")\n",
    "                if not href:\n",
    "                    continue\n",
    "\n",
    "                date_elem = await row.query_selector(\"\"\"p[aria-label*=\"/\"]\"\"\")\n",
    "                published_str = await date_elem.inner_text() if date_elem else None\n",
    "\n",
    "                published_dt = None\n",
    "                if published_str:\n",
    "                    try:\n",
    "                        if published_str.count(\"/\") == 1:\n",
    "                            published_dt = datetime.strptime(f\"{published_str}/{datetime.now().year}\", \"%m/%d/%Y\")\n",
    "                        elif published_str.count(\"/\") == 2:\n",
    "                            published_dt = datetime.strptime(published_str, \"%m/%d/%Y\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not parse date '{published_str}' for {href}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                if published_dt and published_dt < cutoff:\n",
    "                    old_count += 1\n",
    "\n",
    "                if published_dt and published_dt >= cutoff:\n",
    "                    episode_url = \"https://app.podscribe.com\" + href\n",
    "                    page_episodes.append({\n",
    "                        \"url\": episode_url,\n",
    "                        \"published_date_str\": published_str,\n",
    "                        \"published_date_dt\": published_dt\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Skipping episode with date {published_str} ({published_dt}) for {href}\")\n",
    "\n",
    "            # download transcripts one by one in new tabs\n",
    "            for ep in page_episodes:\n",
    "                try:\n",
    "                    transcript_page = await context.new_page()\n",
    "                    await download_podscribe_transcript(transcript_page, ep[\"url\"], save_dir, ep[\"published_date_str\"])\n",
    "                    await transcript_page.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {ep['url']}: {e}\")\n",
    "\n",
    "            print(f\"Page {page_num}: {old_count} episodes older than cutoff.\")\n",
    "            if old_count >= old_limit:\n",
    "                print(f\"Found {old_count} old episodes on page {page_num}. Stopping pagination.\")\n",
    "                break\n",
    "\n",
    "            # go to next page (if exists)\n",
    "            next_btn = await page.query_selector(\"\"\"button[aria-label=\"Go to next page\"]\"\"\")\n",
    "            if next_btn:\n",
    "                is_disabled = await next_btn.get_attribute(\"disabled\")\n",
    "                aria_disabled = await next_btn.get_attribute(\"aria-disabled\")\n",
    "                if is_disabled or aria_disabled == \"true\":\n",
    "                    print(\"Next button is disabled. Stopping.\")\n",
    "                    break\n",
    "                else:\n",
    "                    sleep_time = random.uniform(1.5, 20)\n",
    "                    print(f\"Clicking next page... (sleeping {sleep_time:.2f}s)\")\n",
    "                    await next_btn.click()\n",
    "                    await page.wait_for_timeout(int(sleep_time * 1000))\n",
    "                    page_num += 1\n",
    "            else:\n",
    "                print(\"No next button found. Done with pagination.\")\n",
    "                break\n",
    "\n",
    "        await browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f662a-06ea-468f-b347-2b426b3c2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring safe filenames\n",
    "def safe_string(title):\n",
    "    title = title.lower()\n",
    "    title = re.sub(r\"\\s+\", \"-\", title)         \n",
    "    title = re.sub(r\"[^a-z0-9\\-]\", \"\", title)   \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c13a7-8c37-43dc-8c26-37b19f6bdd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate scraping process\n",
    "df = pd.read_csv(\"data\\\\top_50_pods_USA_2025_Q1.csv\", index_col=\"Rank\")\n",
    "pod_count = len(df)\n",
    "\n",
    "print(\"Starting podcast transcript scraping for the top 50 podcasts in the US for Q1 of 2025...\")\n",
    "for index, row in df.iterrows():\n",
    "    title = safe_string(row[\"Title\"])\n",
    "    if not os.path.exists(title):\n",
    "        print(f\"Scraping starting for podcast {index} of {pod_count} ({row[\"Title\"]}).\")\n",
    "        await scrape_and_download(\n",
    "            row[\"Transcript Link\"],\n",
    "            f\"./{title}\",\n",
    "            months=12,\n",
    "            old_limit=10\n",
    "        )\n",
    "        sleep_time = random.uniform(30, 600)\n",
    "        print(f\"Scraping completed for podcast {index} of {pod_count} ({row[\"Title\"]}).\")\n",
    "        print(f\"Sleeping for {sleep_time:.2f} seconds...\")\n",
    "        await asyncio.sleep(sleep_time)\n",
    "    else:\n",
    "        print(f\"Podcast {row[\"Title\"]} already exists.\")\n",
    "        print(\"Skipping...\")\n",
    "        \n",
    "print(\"Transcript scraping COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
