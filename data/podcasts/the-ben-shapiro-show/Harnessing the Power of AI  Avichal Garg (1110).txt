
Stripe helps many of the world's most influential companies grow their revenue and build a more profitable business. Whether it's Hertz making, checkout a smooth ride for their customers, OpenAI answering unprecedented demand, or PGA chipping away at back office Inefficiency. Stripe's financial infrastructure platform helps companies achieve ambitious goals. No matter what success looks like for your business, Stripe helps ensure the complexity of financial systems doesn't get in your way. Learn more@stripe.com


And bring it back to the, the crypto conversation we're having earlier. I mean, this is, you see this in the data. You can look at Pew data or Gallup data, any survey data, and you see, you know, do Americans or, and frankly to all Western democracies, you know, how much do you trust your public schools? And it used to be in, in the 1970s, it would, it would be 70% and now it's down to 30%. How much do you trust newspapers? It was 70%, now it's down to 30%. How much do you trust the banks? It was 70%. Now it's down to 30%. So you can go through basically every institution in society that we sort of looked to to, to make the system work. And, and it's just inverted from, from the sixties and seventies. Avichal Garg is a successful venture capitalist, serial entrepreneur and founding partner at Electric Capital, where he focuses on investing in innovative technologies in the Web three space, Gard's background in computer science and engineering at Stanford led him to executive roles at Google and Facebook at Gar emerge from the legacy companies of Silicon Valley, a strong advocate for decentralized technologies like cryptocurrency.


Garg views crypto as a democratizing force in tech, enhancing the average citizen's access to digital and financial services at Electric Capital. Garg is now carving a path for early stage startups in crypto, blockchain and software infrastructure to build foundational systems for the future. In today's episode, Avichal outlines the problems with legacy Big Tech and cryptocurrency s role in the declining power of the American dollar. He also talks about why he's an optimist about the burgeoning AI revolution and how we can best prepare for the changes it will almost certainly make in our everyday lives. Stay tuned to hear Al Guard's perspective on the intersection of technology and politics on this episode of the Sunday Special Hial, thanks so much for stopping by.


Really appreciate it. Good to see you. Thanks for having me. Yeah, Absolutely. So, you know, your specialty is the crypto world and crypto. So for people who have like no background in crypto, what is it? How does it work? Why is it important? Yeah, so it depends how far you wanna go down the rabbit hole. But crypto, well, the old school version of crypto is just cryptography. And, and all that is, is is math that lets you secure data and let prove facts about the world. And it's, it's actually really important to the, the, the internet in general. It's why we can do things like credit cards on the internet. That's cryptography. Crypto as it's become to be known over the last decade is, is really how do you use cryptography and distributed systems really to move money around and do new kinds of computation. And so this really started with Bitcoin circuit 2009 and that's sort of kicked off the, what we call the modern crypto.


And that came out of a sort of a, an observation or, or sort of a really a, a, a political belief that the, the legacy financial systems were, were messed up and broken. And so if people go back to 2008, you know, the, the economy was falling apart. The, the banks had sort of done a, a bunch of things that they shouldn't have been doing. And so there was a computer scientist or a scientist by the name of Satoshi, it was a sort of a, a pen name. And they published a white paper articulating how you might do peer-to-peer cash without the need for a bank in the middle. And so how could I transfer money and transfer value to, to somebody else without, without a bank in the middle? And it actually was a combination of both computer science, like legitimate technology breakthroughs and some, some really interesting social design.


And kinda the fundamental thing that that, that they did as a trade off. They said, well, the legacy systems are really designed around efficiency and what if instead of saying, I'm gonna optimize for efficiency, I optimize for security. And so you optimize for things like self sovereignty, I'm gonna open, I'm gonna own my own data and I don't want to trust an intermediary. And so rather than trusting these banks, I I choose to not trust them. Could I do the same things? Could I, could I actually move my money around? And it turns out you can, and it was, it was kind of a crazy idea. It was published anonymously, a white paper was put out, some code, open source code was put out and it started to take off. And you know, here we are almost 14 years, 15 years later, and that core sort of insight around Bitcoin, it's now almost a trillion dollar asset.


There's been a whole sort of explosion of other crypto assets around that and cryptocurrencies and crypto projects around that. That whole ecosystem is worth, you know, somewhere around $2 trillion at this point. And so, you know, a lot has evolved over the last 15 years, but kind of the core original insight was what if I wanted to do things and I don't trust these intermediaries anymore. Which I think touches on a lot of sort of things that people have really started to feel over the last five or 10 years in particular. But this was sort of the computer scientist version of, if I wanna do the things I wanted to do before, how would I do them without trusting any intermediaries? So you can certainly see why established institutions would not like this. 'cause it's quite disruptive if you're, if you're the banking industry and you're used to people using you for those mechanisms of exchange no longer that's being used, you're not gonna like that if you're, if you're a government, you might not particularly like it because you've been in charge of the monetary supply.


And, and now when you're looking at something like Bitcoin, it has its own established value of value established by the market. I want to get to sort of the institutional opposition to Bitcoin. Yeah. And, and, and to crypto generally. But first, what, what do you think that that crypto has has become? So there was a, there was a lot of talk early on about wide adoption of crypto as a mechanism of exchange as opposed to a second property of it, which has a repository of value that basically it's like digital gold. I'm, I'm hedging against inflation in the, in the economy. And so I buy a bunch of gold, I put it in my safe and you know, a year from now it's worth more money. Some people have used Bitcoin like that, but there are obviously other uses for, for crypto. Yeah. What are the various uses for crypto?


Yeah, so what it's really become is sort of from that original insight of potentially the ability to move money around. I think people started to say, well actually this is just a new computational infrastructure. What it really does is, you know, like if you look at Amazon or Facebook or Google, they really built their entire infrastructure around speed and efficiency and scalability. And, and so you got massive centralization, which you see on the internet. You got these intermediaries like Facebook or Google or Amazon that sort of concentrated all of the users and concentrated all of the money and all the power and, and it's partly an outgrowth of what the infrastructure lets you do. So the way the internet was designed, you sort of got this necessary centralization effect because scale sort of makes things more efficient in this world. If you start from an assumption of, well, what if I don't want centralization?


What if I don't wanna trust my counterparty? And what if I'm willing to trade off some efficiency to get these sorts of properties? You sort of start from a different premise. And so the infrastructure has grown in a different way. And Bitcoin was just the first application, this idea of a, of a non sovereign store value, almost, almost like a digital gold. And as the, that sort of started to take off, people started to say, well, you know, what if you could write code around this, what if it's not just about transferring the value, but what if I could? 'cause you know, as soon as the, as soon as the money becomes ones and zeros, well ones zero, ones and zeros is, is just, you know, a, a thing that a computer can recognize. And so what if you wrote code around that? And so starting in about 2015 with Ethereum, people started to say, well, if I can write code around money, I can now program money.


And when you start thinking about a lot of the world, a lot of the infrastructure of the world, whether it's wills or trusts or escrow agreements, if, if you bought a house or insurance securities derivatives, like what are all of these things? Fundamentally what they are is here's a, here's a pile of resources, here's a pile of money. And I have some rules around who can access that money. And I have some rules around what happens over time with that money. There's cash flows that come off of it or you know, in the event of my passing something, something needs to happen with this, this pile of resources. And now instead of all of that being written in legal documents, we could write it in code, we could write in software. And that insight sort of kicked off an an entirely new sort of, you know, arm and branch of, of what's known in crypto today that started to explore this idea of what would it look like if you could write code that owned money and and operated around money.


And so now the kinds of use cases that, that you're starting to see that have really taken off are, for example, stable coins. And this is the idea that actually a lot of the world wants to transact US dollars. And so, and it's quite painful to do that if anybody's actually, you know, the example we always use is if you wanna get money from let's say San Francisco to London and it's Friday afternoon, you are actually better off going to the bank, pulling out $20,000, putting it into a briefcase, buying a flight, getting on a plane and going to London and then handing somebody the suitcase. Because if you send a wire, it's not gonna get there till Tuesday. Right? The, the banks are closed on the weekends, they don't settle wires. It'll take a, you know, t plus one to settle on Monday. And so maybe the money shows up on Tuesday and you could actually have the money in a suitcase there, you know, Friday night or Saturday morning.


And that's kinda crazy, right? In, in, in 2025 almost here, the, there are very few parts of the world where, where atoms actually move faster than bits and money continues to be one of them. And so now you can actually send money via USD stable coins and it's 24 7, it shows up instantly, it shows up in somebody's wallet and then they can off ramp it back to their bank account when the banks open. But it's actually the, the best, fastest, cheapest way to move US dollars in the world now. And so you're seeing on the order of about a trillion dollars, a quarter of money movement happening with this stuff. And, and for a lot of sort of international use cases. So you're seeing, for example, a lot of remittance volumes start to go through stable coins in the Gulf or, or in Southeast Asia where a lot of people want dollars and they can't get access to dollars and there are large populations of people in in the west from, from those markets.


And it's just a better way to do business. I mean, like we, we use this for investing in startups because we can just send money, you know, over the weekend after you invest in a company. And, and so you're starting to see now businesses start to look at this and adopt this as a, as one of the, one of the novel and interesting things. And so once you realize that you can actually, you know, money is digital and you can write code around money, it opens up this whole new universe of, well, what are all the things that, that were technology built in 1970s and we can do those things in a drastically different way today. And so you're starting to see new emerging use cases even beyond just the basic sort of US dollar money movement into things like lending into things like stable swaps, so like Forex sorts of use cases, like really sort of base infrastructure that that we haven't really had to think about since maybe World War ii, you know, Brenton Woods and in the 1950s where we invented a lot of this infrastructure, but we're re-imagining it now with 2024 computer science, which is just faster, cheaper and better, more scalable.


So, I mean, all of this sounds pretty amazing, and yet there's tremendous regulatory heartburn Yeah. Over, over all of this. So, you know, to, to sort of steelman the, the regulatory worries, what exactly are people most worried about that they are now suggesting there needs to be a heavy regulatory regime around some of the use cases or around the original technology? Yeah, So, so the backdrop here, so I think it's important to also contextualize this globally. So it turns out the US is a bit of an outlier in this. Actually most of the world is, is very, very happy to have these technologies. And I think it's, it's an interesting history lesson, which is if you go back to the nineties and the US internet, you know, this, this section two 30 that sort of happens and it was, you know, as part of the telco act of 96. And it was a really, really important piece of legislation because what it did was it opened the door to the internet, the modern internet as we know it.


And, and really what it said was, you know what, these technologies are a little bit different. They're not exactly like newspapers that have editorial staff and, and you can sort of control these things in the way that you did the newspapers. We gotta think about these rules differently. And so, for example, it said aside rules, it said, well, you know, if you're a digital forum like a Facebook or YouTube, you have different rules than a newspaper might have. And, and actually like if you, you know, as, as a, as a, as a media person, you don't need to go to the FTC to, to get a, a media license, a radio license to, to publish on the internet. And so these sort of basic ground rules understood that the technology was different in some fundamental ways. And so we needed some new rules to account for how it was different that ushered in the modern internet. Like the reason America won the internet was because some of these rules were put in place in, in the late nineties.


And so we got Amazon, we got Facebook, and we got YouTube, and we got Google and, and all the sort of mega tech companies. Now, the rest of the world looked at this and today has a set of challenges. Like if you're, if you're Europe, if you're, you know, Brazil, if you're India, if you're China, you're looking at these things and you're saying they're so, so, so important in society as sort of of fundamental pieces of infrastructure, we have zero leverage over them. Like, you know, if you, if you want to call in one of the CEOs of these companies and you're Europe, are the CEOs gonna come? No, they're American, they're American citizens, they're in America, like they're subject to US law. They will, they will abide by most of these laws in, in these other places. But you, you have relatively little leverage actually over, over these companies because fundamentally they're American companies.


So it's a, it's a tremendous national security asset that these ended up being American companies. So now the rest of the world looked at this and said, oh, we, we kind of lost the internet. We don't wanna repeat that mistake. And so if you look at what the rest of the world is doing around things like crypto or ai, they're really leaning into it. And, and what they're trying to do is figure out how do we make these companies successful in our domestic markets? Because if we can do that, we'll actually in the long term have leverage and we won't be beholden to these American companies over whom we have no leverage. So actually the rest of the world has gone very pro crypto, uk, Germany, Singapore, many of these markets are actually embracing it and leaning into the technology they're giving visas to, to people to buy, they're giving visas to, to people to come build these companies.


United States, on the other hand, I think we learned some wrong lessons, which is because we were the beneficiary, it's almost like a resource curse. We sort of won the internet and we forgot in many ways why we won and we sort of take it for granted and how powerful these these entities are and how important they are from, from an economic perspective and a national security perspective. And so we're kind of, we're kind of botching it a little bit. And so there's a lot of sort of dragging your feet, there's a lot of opposition, there's a lot of sort of fear about what these things might do rather than sort of optimism about how, how amazing things could be. And so the US is a bit of an outlier. And specifically in the US a lot of the concern fundamentally comes down to, in my opinion, a philosophical concern, which is who gets to control these things?


And I think from a government perspective, you know, the the legacy financial system and a lot of these computational companies like an Amazon or a Google, well, you can call the CEO, you can, you can have 'em come testify, you can yell at them, you can, you can get your PR moment or you can, you can find them. How do you do that with a, a decentralized crypto network? How do you really think about that? And, and the reality is you can't, there's no, who are you gonna come yell at about Bitcoin? You can't. And even if you could, there's nothing they can do about it, right? The code is open source and it's running and it's, it's autonomous. And this presents a real challenge, I think, and we don't really have a great framework for how to think about that. I think the last time in my opinion that we had to think about these kinds of issues was, was frankly like the invention of the LLC like in the 1850s.


So if you go back and look at the history of, of limited liability corporations, and we invented it was, it was actually a fantastic technological breakthrough. Something like the Dutch East India company, which said, let's pull a bunch of capital and we'll go pursue this venture. And that that charter, you had to go get that charter from the king. So the king would give you a charter to start a corporation and you could pool assets and then go pursue risk opportunities. And, and it was really a separation of sort of capital risk from the people pursuing that with their skills. It took about 150 or 200 years. But a lot of people started to say, well, wait a second, why do you need to go to the king to, to talk about these things? Like why, why can't I, I just start a company. And so around 1850 in the UK and the US we, we created really simple ways for people to create corporations.


And it's actually really fascinating if you go back and look at some of the, the debates that were happening at the time, there were really interesting questions like, well, you know, are these things people, like, do they have rights? What happens if there is a fire and somebody dies? Like, who's liable? Is it the owner of the thing or is it the company? Or who pays? Will these things have free speech rights? Will they be able to like, participate in elections? What does that mean? Will they have too much power and money? And they're like, actually a lot of really interesting questions that were surfaced at the time. But what we said was, you know what, this is a useful new technological construct and so let's create some rules around things like corporations and LLCs. And that was a really big breakthrough because that was a big part of what allowed things like railroads or the industrial revolution, because now you could aggregate capital to pursue opportunities at a scale that you just previously could not.


And that sort of fundamental set of questions of like, how does this work and what are the rules? People had to really sit down and think about things like liability and ownership, free speech rights and all these things. We haven't really had to face that for 150 years. You know, we, we we are now faced with a similar set of questions here around, well these are decentralized networks. There isn't a CEO that you can just call. So how does liability work? You know, how how does free speech work? Like if, if I can write code and just put it on the internet isn't have free speech, what happens if something goes wrong? How do we think about things like KYC and anti-money laundering and how do we prevent terrorist financing? There, there's some really fundamental questions here that we don't, you know, we need to sort of sort through.


I don't think the answer is to just say no, because the reality is that that's not an answer because this stuff is happening globally now. And so I think the US has actually sort of dragged its feet and, and there's been a lot of opposition, but I think it's actually to the detriment of of the US to not have, you know, people thinking about these and questions and trying to, trying to answer them.


This past year we've witnessed not only the ongoing war in the holy land, but also something even more disturbing in some ways an absolute explosion of antisemitism across the west when Jews in Israel need support most, I'm proud to partner with an organization that's been there for decades. The International Fellowship of Christians and Jews for over 40 years. The fellowship has been building bridges between Christians and Jews. Their work has never been more critical than it is today. Right now they're on the ground in Israel and Ukraine providing real help, food security and essential supplies to those who need it most. Your support during this critical time makes a concrete difference. Every gift you give to the fellowship goes directly to helping people with food necessities and security measures that save lives. Here's what I need you to do right now. Go to Ben for the fellowship.org. That's been for the fellowship.org. Make a gift today in the face of these many threats, the fellowship's ongoing work, providing security to Israelis has never been more important. Remember, that's been for the fellowship.org, they're doing important work on the ground in the holy land every single day. A lot of suffering people there give generously head on over to Ben for the fellowship.org. God bless and thank you.


So when, when, when you look at sort of the current regulators and the regulatory environment, gimme some examples of some, you know, badly thought out legislation or badly thought out regulation. You know, obviously if you're, if you spend any time in this world at all, even even like me sort of casually the name Gary Gensler comes up a lot. Yeah. They, they're, they're, you know, regulators who are, who are widely, shall we say, disliked Yeah. In, in sort of these communities, in the crypto community for, for being heavy handed, for, for believing that, that you can sort of put the genie back in the bottle. What does bad regulation look like? What are the problems? Yeah. Well, bad regulation is actually no regulation. It's regulation by enforcement. And so there's no legislation. And so what we really need are, are are clarity of, of the rules.


And it's actually, I've never, it's, it's pretty crazy. I've never worked in a, in a space that is demanding legislation the way that the crypto industry is. Like, people go to senators and Congress people and they're like, please regulate us. Like tell us what the rules are. 'cause we wanna follow the rules. Because in the absence of these rules, what you really end up doing is empowering the bad actors, right? What you end up doing is the people who are willing to skirt the law, who are not willing to spend time working with the regulators or the legislators to figure this stuff out, are the ones who just go off and start, you know, doing scammy things and then, and then people ultimately lose their money. And so for example, if you look at, you know, the, the really famous example of this from a few years ago is FTX, which is, you know, of sort of an American company. It was a bunch of American founders, but they moved to The Bahamas to run their business. And so it looked like an American company, but in practice was not an American company.


It was not regulated by the United States. They were not subject to disclosure rights in the US but a lot of people trusted them 'cause they looked like an American company and it ended up being a $10 billion fraud and Sam b fried is, is going to jail and so on. But that's a perfect example of the lack of legislation and clarity creates the opportunity for these sorts of bad actors to come in and, and exploit the system. Meanwhile, the good actors are trying to figure out how to comply and it's unclear how to do that. And so when it comes to some of these agencies, what you really see happening is that they are taking this ambiguity and, and in the absence of clarity from the legislators are essentially deciding the rules and dictating those rules. And sometimes those rules are applied arbitrarily and capriciously.


These are literally words that judges have used and, and a handful of companies that have the resources are now taking, taking the SEC to court and, and winning. And it's, it's actually pretty remarkable. I mean, it's, if for anybody who's studied the history of the agency model and Chevron and sort of these, these recent rulings that have happened, it's, it's pretty fascinating because historically, you know, if the SEC gave you a wells notice or said we're taking you to court, their hit rate was so good, they were so trusted. Like your stock would tank if you got a wells notice these days because they, they've taken such a different strategy of, of issuing wells notices and, and frankly losing in court multiple times, the market just doesn't even take that seriously anymore, which is a real, I think hit to the credibility of the agency at the end of the day. And so what's happened is we, we actually don't have legislation, we don't have regulatory clarity.


The agencies are trying to do this through enforcement, and then when it goes to court, they're in many cases losing. And that's just an extremely expensive way to, to run an industry. And so this is why the industry is going to, you know, there are many great legislators on both sides, on, on Democrats and Republicans, you know, Senator Gillibrand from, from New York, you have Richie Torres from New York on the Dem side, you have Roanna here from California, you have Patrick McHenry, who's the chair of the House financial services committee. You know, you have people who are, who are trying to push this forward, but that's what we ultimately need is we need, we need some sort of legislation to start clarifying what the rules actually are and then, and actually telling the agencies what the rules are because in the absence of that, the agencies are sort of are, are, are, you know, taking this into their own hands.


So you mentioned economic growth and innovation. Obviously the, the area that's seen tremendous amounts of investment right now is, is in ai. So let's talk about ai. Forget a lot of people, very, very nervous about ai, about what it's going to do. Yeah. And there's sort of the tech optimist view, which is it's an amazing new technology. It's going to allow for innovation at a level that we've never seen before. Because essentially at a certain point you are gonna reach artificial general intelligence, which means effectively, why, why don't you to define, I don't wanna mystify, its why don't you define what, what artificial general intelligence is? Yeah, well it's interesting because a GI sort of has, has a couple of different flavors of it. I think one version of it is, well there there's another test we talk about, which is the Turing test. But a GI generally, the way people will describe it is for an arbitrary task, this software could perform as well as an arbitrary human.


So, you know, an average typical human. Now there's a lighter weight version of that that you might say, well for some universe of tasks will this outperform? So it it's not universal in the sense that, you know, a human is very good at learning how to drive a car and, and ride a bicycle and also do their taxes and also learn email and learning software. And so maybe you don't get a GI that isn't necessarily good at doing all of those things, but you might have some sort of human level intelligence that for certain types of tasks it does as well as human. I think we tend to get, we're we're more likely to be able to get that level of intelligence relatively quickly. The broader, like it can learn anything with very limited data. You know, humans are phenomenal with a couple of data points. You know, like you can teach a 4-year-old how to ride a bike in a couple hours.


It's pretty remarkable that we can do that. That I think is, is some time away. And so if you define a GI that way, that's probably many years away. If you define a GI as four certain types of intellectual tasks, it will do as well as a typical human. I think that's, that's on the order of like two years away now that whole sort of can of worms opens up a whole set of questions. I think this is the, this is the white collar analog of what happened in the eighties with manufacturing as robotics sort of entered and has real consequences on society. Because I think there will be something, if I had to guess something like 50 to 70% of jobs that people do today, a computer will be better at doing in two to three years. And so what this means is there's real economic pressure for businesses to not have as many employees to get the same output over time.


I don't think this means there are fewer jobs. I think this means that companies will do more with fewer people. And so you can actually do 10 x as much with the same staff, but it does present some challenges in the short term, which is that a lot of these jobs that humans are doing today take things like, you know, doing your taxes, you know, scheduling emails, you know, think about the number of financial transactions that you do that involve just filling out some PDF and sending it back and forth. Like a huge swath of the economy actually computers will now be, will be better at than humans. And we don't know kinda what the consequences of that will be societally from a productivity perspective and a growth perspective, it's gonna be phenomenal. Like it's, it's gonna make every business that much more efficient. It's gonna make, you know, the economy boom and sort of take productivity through the roof, I think.


But the second order effects I think are still a little bit TBD. Yeah. So when, when we get to the second order effects, which is I think what, what people are mostly worried about Yeah. The societal effects of what happens when, you know, half the population, at least in the temporary term hasn't adjusted yet to the, to the shift because yeah, as you mentioned, if you look at the history of American, you know, economic growth, there've been a number of industries where it used to represent a huge percentage of the workforce and now it represents a tiny percentage of the workforce and people moved into other jobs. But there is this lag effect. I mean, America used to be a heavily agricultural economy. Yeah. There was a heavy manufacturing based economy. Now it's a heavy service economy. And so when that, you know, is taken over by ai, when, when AI is doing a lot of those tasks, what do you think is the, is the sort of next thing that that human beings are still going to be better at than machines?


Like pre presumably there'll be a cadre of experts in every field who will still be better than the machines are because they'll be the geniuses Yeah. Who are a, you know, if you, you're a, you're a Nobel Prize winning nuclear physicist and you have a bunch of grad level students who are working for you, but they're all ais Yeah. It wipes out the grad level students. It doesn't necessarily wipe out the nuclear level, the the Nobel Prize winning Yeah. Nuclear physicist. But you know, if you're not that guy Yeah. If you're everybody else, which is, you know, 98% of the population. Well, what exactly do, do you think is left for those people to do? Yeah, I think a lot of it will become to human interactions. So there are certain things that humans ultimately will still want human interfaces for. So for example, going to the doctor. So an AI is gonna be better at diagnosing you.


AI is already better at diagnosing than than most doctors today. But there's something fundamentally human about going to the doctor's office and having somebody talk to you and listen to you, that doesn't go away. And so the, the modality of how you do your job in, in a lot of human to human interactions will change pretty fundamentally. And you'll have to retrain a bunch of people on how to, how to use these tools properly. But I think what you'll see is anything that involves a human, you know, real world interaction, I think will probably persist. And then beyond that, I think it's, it's a big open question. You know, I think this is a big, big economic alignment. And so for example, what does, what does education mean in this new world? Yes, teaching is still a very human thing, but how do we prepare our students for this new world?


Or how do we think about, you know, you're talking about the, the Nobel Prize winning researcher. I think it's entirely possible that in seven to 10 years for some of these domains that the AI is actually smarter than the Nobel Prize winning, you know, physicist or whatever. And so when you get the superhuman intelligence on the other side of that, what are, what are the consequences? And, and I don't think anybody really has a great answer to that yet, other than the human systems will persist. Anything that involves information, anything that involves data, anything that involves computer code, the AI is probably gonna be better than human that in, in relatively short order. So that, I mean, that, that sounds pretty scary too to a lot of folks, obviously. I mean, especially because we're an economy that is optimized for lack of a better sort of term for iq or we're an economy that is optimized toward intelligence.


And now you're saying that basically intelligence is no longer the metric for success because it's gonna be available to everybody in the same way that the resource manufacturing resources have now become unbelievably available to everybody. You don't have to have a manufacturing facility to get anything you want at your front door in a day. Yeah, it's, it's gonna be like that except with intelligence. And so what the, you know, the sort of where the mind goes is to places like, okay, so now you're gonna, you know, be optimizing for emotional quotient, right? You're gonna be optimizing for how well people deal with people. If you're, if you're an extrovert, you're gonna do really well in this new economy. If you're an introvert, you might be a little bit screwed. Yeah. Right. The engineer types are gonna have a real tough time. But if you're somebody, if, if you're a, if you're a face for the, for the AI to teach kids, yeah, then, then that'll be okay. It also raises serious questions, presumably about the educational process itself.


What do you even teach kids when, and I I see this even with my own kids now, right? I mean, like, I, I son who's a smart kid, I'll bet he was delayed by about a year because he understood how voice to text worked. He just grabbed my phone instead of actually typing in the question the way that, that he might, if you were, if you were able to read, he would just, you know, actually just voice the question and then he would have the, the phone read it back to him. He never actually had to had to do that. And so what exactly are we going to be teaching kids and how does that affect the brain? Because obviously, I mean, these are all open questions when it comes to like, it, it seems to me that you stop teaching kids math, for example, because it's so easy to just do everything using ai. And doesn't that atrophy a part of the brain that you might actually have to use? I mean, there, there are current general uses that, that are taught via specific skill sets Yeah.


And that apply more broadly. Yeah. All excellent questions. I don't think anybody has the answer. I think there are two ways to, to sort of think about it. Two data points you might consider. So one is, a lot of this was sort of what happened on the internet. So imagine you're in 1990 and you can see the internet coming. You can see e-commerce coming, you can see video conferencing coming. You can, you can see social media coming. What would you do? That is the question I'd be asking myself if I were a parent right now, if I were a teacher, if I were a white collar worker, if you understood that the internet was happening in 1990, what would you have done? And I think probably what you do is you start getting really, really familiar with the technology. You start writing emails, you start buying stuff online. You start, you know, buying PCs and, and sort of staying on top of it so that by the time you're in, you know, the year 2002, you've, you've sort of upskilled yourself.


You've gotten to a place where you understand what these like tools are capable of, and you've reskilled and so you're, you're able to sort of move into this new world. You know, could anybody have predicted that, you know, in 2010 or 2020, you would have, you know, the entire media landscape would look fundamentally different than it did in 1995. I think that was a big leap. But if you understood the tools, you, you were well positioned to sort of move into that, into that new world as it happened. So I think that's one mental model you might have is what happened in the nineties and what would you have done if you, if you knew everything that you knew about the internet today, but you're in 1990, what would you do? And I think that's probably the best you could do is let's just get really, really familiar with the technology. Let's make sure we buy our kids a pc. Let's make sure that they play with it every day and, and by the time they're 10 or 12 or 15, they'll have an intuitive sense for how this technology works.


And then they'll be okay. The second thing I would observe is that all of these transitions are very scary in the moment, right? Whether it's the internet, whether it was PCs, whether it was, you know, factories, whether it was railroads, whether it was the car, all the way back to the printing press, right? And, and there's always this fundamental debate that happens. There's always this fundamental question that happens between two, two groups of people. One group of people says, you know what? The current system is fine. And, and actually we don't need this technology. We don't want this technology because it's gonna be so disruptive that a bunch of people will be left behind. And there's another group of people who say, no, no, no, as scary as this thing is, it's a tool that will make life better.


And I think every time you look at the tools that humanity creates, if we choose to adopt them, life gets better, right? So it takes something like the printing press. It was a big driver in why we have literacy. And I think you would be hard pressed to argue that, that people being able to read is a bad thing. But there are actually people around that time that said, actually learn to read is a bad thing. We don't want people to learn how to read. This creates a negative social consequence. You know what if people can actually read the Bible and understand what it says, like that's really dangerous. We don't want that. But I don't think anybody would make that argument today. And so I think ai, we're probably gonna look at similarly in 20 years, I think we're gonna say, you know what? Like, yes, it was a transition, but on balance it was extremely positive. And, and it does create some societal questions about how we manage that transition for people so we don't end up with a rust belt the way that we sort of, I think, abandoned a bunch of people in the eighties and nineties and didn't re-skill them.


And, and this is a, this is a problem that we need to think about, but I think on balance it's gonna be extremely positive. And, and the best you can really do is just immerse yourself in it. And so, you know, for example, at work and in our company, what we've done is we literally, every person is required to use an ai. And, and every day you have to use it. And we share productivity tips. And so we have a, a slack channel where people are sharing these tips. We have an engineering team internally and, and we task them with going and, and actually getting these open source models and running the code and, and actually trying to build agents that, that will be able to do the job of a finance person or an operations person so that we can scale our business 10 x without hiring more people. And so I think the people that just sort of jump in the deep end today will be at a, a tremendous advantage in two to five years.


Alrightyy folks, let's talk about dressing sharp without sacrificing comfort. If you're tired of choosing between looking professional and feeling relaxed, I have great news for you. Collars and Co is revolutionizing men's wear with their famous dress collar polo. Imagine this, the comfort of a polo combined with the sharp look of dress shirt. It's the best of both worlds giving you that professional edge without the stuffiness. Longer are the days of floppy collars that make you look like you just rolled outta bed. These polos feature a firm collar that stands up straight all day long. The four-way stretch fabric means you can freely move comfortably throughout the day it's office approved. So you can look professional without feeling like you're trapped in a suit and get this, it travels really well. So whether you're commuting to work or jetting off for a business trip, you'll arrive looking crisp and feeling great. But Collars and Co isn't just about polos. They've expanded their line impressively. They've got merino sweaters, quarter zips, stretched chinos, even a performance blazer they call the maverick. It's versatility at its finest. These pieces look great by themselves under a sweater or with a blazer. Look at what I'm wearing right now, don't I look spiffy? I mean, check out this jacket. You see this thing? This thing is great. So if you wanna look sharp, you know, the way that I do feel comfortable, support a fast-growing American company, head on over to Collars and Co dot com, use the code Ben for 20% off your very first order. That's Collars and Co dot com code ben Collars and Co, because you shouldn't have to choose between looking good and feeling good.


So a couple of the challenges that people have brought up as possible theoretical challenges is one is called the data wall. Yep. The basic idea being that what happens when right now LLMs large language models are, are learning based on the amount of data that's on the internet. What happens when people stop producing the data because the LLMs are actually producing all the data and then they're really not producing the data, they're synthesizing the data that's already there. So do we hit a point where, because there's no new human created data that's actually being input, that basically the internet runs out of data? Yeah. And there are some people who sort of suggest that that's not going to happen. There are some people who suggest that it will. What, what do you make of that question? Yeah, It's a very interesting question. I I tend to think we will probably not run out of data anytime soon. I think there are large domains that are private data that are not yet fully tapped.


And so I think when you think about things like medical records, bank records, financial data, government data, there's a lot of data actually that's not yet been ingested into these systems. I also think that the AI creating stuff, it does create sort of a self-referential feedback loop, but I think a lot of that data is quite valuable. And so you will have a lot of quote unquote AI generated data or potentially even synthetic data. And I think if you can put a layer on top of that to have some sort of filtering that says actually this data is good data, you might actually get really interesting, you know, feedback loops that actually the, the AI can sort of learn from the ai. So I think it's a big open question. I, I'm sort of on the side of, actually, I think we'll get better and better at using the existing data that we have and there are still large pockets of data that are untapped. And so I don't think we've hit sort of the, the top of the s curve in terms of being able to hit, you know, diminishing returns with data yet.


And When it comes to the, another question I've seen raised about this is is the question of innovation. How, how much can AI actually innovate as opposed to sort of performing sophisticated recombination, which I, I, I suppose actually goes to what exactly is innovation? Is it just sophisticated recombination or is it something else? It's sort of like you're in the shower in the flux capacitor hits you and you fall off the toilet. Or like, what, what, what exactly is that? How far can AI go in terms of, of what we would perceive to be innovation? Also because whenever there's a sophisticated technology, human beings tend to then map their own model of the brain onto the sophisticated technology. So for a while it was the brain is a computer. Yeah. And now it's the brain is in ai. Yeah. And and how accurate is that? Yeah, I think it's, it's a, it's a tricky question.


I think there's gonna be a class of innovation that this stuff is really good at, which is when you have large amounts of data and you can interpolate inside those data sets, you can infer a lot. You can learn new things. And so you see for example, things like protein folding, we have a lot of data and we can start to learn how these things might work and how these systems might work. I think there's, there's some people who think that actually extrapolating beyond the data, creating entirely new frameworks and new ways of thinking is something that, that humans are uniquely good at. I tend to be on the side of, you know what, every time humans say that somehow we're special and unique, it turns out to be incorrect. And so whether we're talking about, you know, voice, whether we're talking about, you know, we're talking about the Turing test, you know, for a long time, you know, Alan Turing was one of the pi, one of the pioneers of computer science.


He created this test that basically said, if you can talk to a computer and, and maybe that's textual, maybe it's voice, but let's say it's text and you cannot distinguish, let's say you have a black box and you, and you sort of are asking you questions and it, it, it, it gives you answers If you cannot tell if on the other side of it is a human or a computer that is intelligence that's indistinguishable from a human. And we've, we've passed the turn test at this point, right? You can talk to an LOM and it looks like a human for all, you know, it's a human type in that response. And it, it appears to be self-aware and, and cognizant it's not, but it appears to be that way. And so we, we actually passed this really important milestone that, that most people don't talk about, which is the Turing test. And so I look at that and I say, well, there are a whole set of things that humans have claimed for a long time are uniquely human, that these things based in silicon are now doing.


Why if we have an existence proof for a set of things that we thought were uniquely human, that now these computers can do, why would it be the case that there are a whole set of other things that these hu that these, that these computers would not be able to do? So I'm kind of on the side of, it's just a matter of time that, that these things are able to replicate all of these things, including the ability to extrapolate and the ability to create new frameworks and to, to be creative. I think, you know, it may take longer, but I think those things will ultimately fall too. So when we look at the world of AI and tons of money is pouring into ai, you see sovereign wealth funds all over the, all over the planet. People building their own ais. Well what, what exactly does that mean? We say we're building in ai. What does that mean? Yeah, that's a good question. There, there's a lot of pieces to it. The thing that most people ultimately see is some sort of a model that they're interacting with that you can, you know, type in text to or maybe speak to the, the voice transcription models are getting really good and the, and the text to voice and voice to text all that stuff is, all those pipelines are getting really good.


That's generally what people think of. There's a whole bunch of infrastructure behind the scenes that makes that possible. Everything from how do you acquire the data to how do you clean the data to how do you get some sort of evaluation on it to how do you actually train these things on very, very large clusters with lots and lots of GPUs and, and, and doing that at a scale that requires, you know, many, many tens of thousands of GPUs and coordinating all of that. There's a lot of infrastructure and computer science that goes into that. And so that's where all the money is going, right? Ultimately it's, it's how do you take large, large amounts of data, the entire internet and compress it into, into these very, very small models that hopefully can run on your phone one day. And the entire process of doing that is extremely capital intensive because the sheer amount of computational resource and, and data centers and power that you need, the GPUs that you need are so immense to actually produce these, these models that you then interact with as an AI and just requires tens of billions of dollars to be able to actually have all that computational power to, to create these models.


And, and how do you tell the difference between the models, right? I mean, if you're using, are they all using the same data centers or just they're using different data sets? Is it waiting? Like what, what, what is the difference? It's a, it's an excellent question. I think at the end of the day, they will basically all be using the same data sets. And so a lot of it comes into how do you do the training and there's a lot of sort of know-how and, and sort of proprietary knowledge around how you actually train these things. A relatively small number of people in the world know how to do that today. And out of that come these weights and, and, and those weights effectively let you understand what, you know, given a word, what is the, what is given a context and a word, what is the sort of the next word that, that the system would predict. And so those weights in the case of let's say a llama or open source in the case of something like open AI or anthropic or not open source, but ultimately that's what you're trying to produce.


You're trying to produce sort of take all the data, compress it, and the the compression ultimately is producing this sort of set of weights. And the weights are just, all they're really doing is saying, give an word. What is the next most likely word? I, you know, I'm glossing over a lot of details obviously. And, and that's what we call sort of the AI model. And, and there's a lot of sort of how do you do the compression? That's, that's sort of all the proprietary stuff, you know, which, which speaks to actually this, this sort of really, it kind of goes back actually to, to this technology problem we've talked about, which is there, there that stuff isn't, is very opaque actually, even though you can see the, even though you may be able to see the weights on the final model, you actually have no idea the data that it was trained on there that is not disclosed.


And so very subtle biases could be entering into the data that we have no idea about, which presents a real challenge because these are private corporations, they're not subject to, you know, our, our vote ultimately at the end of the day. And so we don't really know what's going in to these data sets. We don't really know what's being kept outta the data sets. And that's, that's all opaque and there's no real accountability at this point in terms of what that means. And so there's, there's a lot of sort of, I think, open philosophical questions here about like who, who should get to dictate what goes into the data sets and why, and what does that mean downstream for, for what these models are. And so there's a real push from, from some in this community to make all of this open source, not just the weights, but actually open source, the entire process. Like, tell me what data you're using. Tell me what data you left out.


Tell me why are you're using this data, tell me what your eval scores are like, and open source, the entire thing. And we'll see if that happens. It's, you know, there's, there's sort of some tricky economic questions there because there's so much money to be made that you kind of don't wanna disclose those trade secrets because you don't want somebody else to be able to replicate this. And so right now, all of that whole, that, that whole pipeline is effectively enclosed. Nobody, yeah. I mean the, The, the closeness of it does create the possibility from pretty dystopian Yeah. Issues. I mean, just to take a quick sort of practical example, I mean, when, when you used to use Google, you would Google a term and then you'd get a series of webpage, and then you sort of, yeah, you prioritize them in a certain way. But you could always scroll the page too and sort of figure out what exactly you were looking for. Now you use Google's AI and it spits out, you know, an answer that it's, it's destroying internet traffic.


You know, nobody has, has used a click through link on Google for several months at this point. And by the time my kids are using it a lot, they won't be using it at all. I mean, it'll just be, they type in a prompt and then the, the answer comes back out. And then how you determine what that answer actually looks like, how they got to that answer is gonna be very difficult. And so you can certainly see the possibility of bias and the informational environment re and obviously, you know, conservatives are worried about this all the time. Remember when, when we were first seeing chat, GPT conservatives were typing in things and it was coming out very left wing. Yeah. And we were saying, well, what are the, what are the parameters that are being used on, on all of this? Yeah. And there's no visibility into that. And, and actually, and you know, the, the argument on the other side is, well, this stuff is really dangerous. We don't wanna open source this stuff. What happens if it gets exploited by rogue government?


Or what if bad actors take this technology and do things with it? And so there's this really strong sort of safety movement around some of this. And I don't think it's, it's misguided necessarily, but I think that the form that it can take is essentially trust us. Like we, we are the arbiters of truth. We will do the right thing, but I don't think that's, that's actually the right way to go about doing these things. And, and, you know, we were, just to bring it back to the earlier conversation we were having, I think that there's actually a, a very interesting historical analog here around, around religion, which is, you know, the, the sort of handful of people that really understand how these systems work and ultimately are making decisions about what data goes into these systems. It's sort of like the church, you know, in, in sort of, you know, this sort of 1400 era where there's a, a, a set of people and they get to dictate what is truth.


And over time, you know, even with the best of intentions, even with the best of people, if these things are long lived enough, there is generational transfer, there are new people that come in. And if there's no accountability, those the systems have a tendency to, to become corrupt. And this is what Martin Luther noted, you know, when he, when he, when he bang those 95 thesis into, into the wall at the church was, wait a second, like, the system has become corrupted in all sorts of ways, things like indulgences, and there's no accountability. And what people on one side of it said was effectively, Hey, look, knowing how to read and read the Bible and understand this stuff is really dangerous. You need an interpreter. You need an, you need a a an intermediary to help you understand how to do this. And trust us, we're gonna filter it in the right way and we'll do what's right for you and we'll do what's right for society.


And there's another argument that said, no, no, no, you should have a direct relationship with God. You should be able to read the Bible and decide what you do. And actually that sort of, the 95 thesis and the printing press happening at the same time was sort of this confluence of philosophy and, and sort of technology coming together. And I think there's a very similar philosophical, technological argument, intention happening right now. Do a, a small group of people get to interpret the data and get to decide what goes into these models, and, and they get to dictate truth. And you, you have very little say over that and whatever they say is true is true. Or do you get to have a direct relationship with the ai? Do you get to have a direct relationship with what data goes into it and how you train it and so on. And there's a lot of people that sort of fall into this world, but it's, it's, it's a scary world because what you have to do if you, if you take that position, is you have to believe that humans are fundamentally good.


That if you give them this technology and you give them this knowledge that they'll do the right things with it on balance. Humans are good, but that's a big leap for a lot of people that it's a scary thing to hand over that powerful technology to, to everybody in the world. So when you look at the risks of ai, you see sort of the catastrophes risks, the idea that it's gonna take over the nuclear missiles and you start firing them at people, you know, how how seriously ought we to take the sort of World War II end of humanity risk, the, the, the Skynet scenario? Yeah, I Think it's probably overblown personally, I think it's, it's, it's unlikely that, that this thing runs away from us in a way that, that we can't, you know, control. I mean, literally you just shut off the data center, right? It's, it's, we have a kill switch.


And so I don't, that's not to say you shouldn't, you know, think about these things and, and, you know, battle harden your systems and so on. But I think there's a lot of sort of religious fervor around this. And I, I think the place where it actually comes from is a desire for control. And it's, it's a fear-based motivation and it's a, it's sort of a degrowth mindset. And I think if you have that kind of a mindset, it's very easy to fall into this trap. And then of course, the question is like, well then who gets to decide the safety? And the answer is, well, me, right? And so it's, it's actually sort of like a, it's a, it's a perverse incentive. I think it's like, by and large, the people that are pushing this argument are, are very self-interested in, in wanting to push this argument. And, and hence I think the argument gets, gets overblown. I think this also sort of feeds, you know, from Hollywood, like I think for the last 20 to 30 years, the, the way that the media has really thought of technology has been through this dystopian lens.


I think it's, it's actually like a really fundamental problem in society. If, if the, the interface that most people have with technology is sort of, they, they use the technology, but then they see media about technology, and by and large that media is negative, then people are gonna have, the gut reaction is gonna be dystopian. Whereas if, you know, if you go back to the sixties, you see there's like beautiful posters that we're gonna have cities on the moon and we're gonna have underwater cities and we're gonna be living on Mars. And, and so there's this sort of like positivity around technology. And so I think a lot of this is actually, it's, it's cultural and societal. It's like, actually I think it's, it's not about the technology. It's actually kind of where are we as a, as a country and as a society. And, and, and I think there's a lot of dystopian sort of undercurrents just generally in society right now. Yeah, it's a fascinating point. You, you can see where that's coming from, meaning lack of social cohesion leads to mistrust of one another, mistrust of one another means we're afraid that the technologies that, that we all control are going to go completely awry and we'll use the technologies to murder one another.


If it were you and your family and you're just like, wow, this is a great new technology that we just adopted in our house, then you're really not super worried that your family is going to blow each other up. Yeah, totally. But it, you know, it, I'm not sure how to, it seems like, you know, very often I come back to this, but, but without, you know, the, the institution of social cohesion coming back, then you will see a sort of revant just ludism that that's going to rise. And I think you do see that on the rise. I think you see people being like, well, this is all moving too fast. This is all too scary. We need to, you know, burn the looms. Yeah, Yeah, that's right. And, and then bring it back to the, the crypto conversation we were having earlier. I mean, this is, you see this in the data. You can look at pew data or Gallup data or any survey data, and you see, you know, do Americans, and frankly, it's all western democracies, you know, how much do you trust your public schools?


And it used to be in, in the 1970s, would it would be 70%, and now it's down to 30%. How much do you trust newspapers? It was 70%, now it's down to 30% how much you trust the banks. It was 70%. Now it's down to 30%. So you can go through basically every institution in society that we sort of looked to to, to make the system work. And, and it's just inverted from, from the sixties and seventies. It, it was that people trusted these institutions and, and, and they don't anymore. And I think it's actually warranted, most of these institutions, if you think about it, we're created after World War ii and they've really atrophied and, and, and it is time for a reboot in a bunch of these institutions. But I think you're right that the, the root of it is social cohesion, right? It's like, how do we think about who we are as a people, as a society, as, you know, what are our values? Because the institutions are a reflection of that. And, and the institution's atrophying and the social cohesion being lost over the last 50 years, I think kind of go hand in hand.


Do You think that technology can exacerbate that meaning that, you know, when, when, when you look at, at crypto is, is actually a, a great example of this in the sense that let's say that you, you know, there are higher levels of social cohesion and you, you did wanna transfer money from say, San Francisco to London, and you wanted to do it over the weekend, but all the banks are closed. Yeah. So in the old days, right? Yeah. Or if you had a close knit community, what you might do is get on the phone with somebody who you knew. Yeah. You'd say, take a certain amount of money outta your bank, that's Friday afternoon, take that money outta your bank now I'll pay back on Monday. Yeah. Right? Like, and, and people, okay, like, I know you, I know your family, I know you're good for it. No problem. Right? And, and as social cohesion breaks down, it's like, okay, we actually need to now control for that by creating trust free systems that allow for that sort of monetary transfer.


And the more you rely on the trust free systems, the less actually there's a requirement of trust societally in order to, in order to get to the same sort of outcome. Hmm. Yeah. That's an interesting take. You know, as a, as a backstory, this is, this is how the Rothchild Empire started actually. Exactly. The brothers all trusted each other, and so you could do this kind of stuff in a, in a, this Was their market mover advantage. It really was. I mean, you had like brothers all over the continent. Yeah. And they would just wire money to one another or send a node to one another in code. Usually it was actually in a Hebrew script, it would be like transliterated English, but in, but in Hebrew. So people think it was Yiddish, but it didn't read like Yiddish. And, and that that's how they had that that first mover advantage. It's still how, what they call, what, what Thomas o calls middleman minorities tend to thrive in most societies specifically because of this. So to take like a, another Jewish example, Hasidic diamond dealers, right?


Everybody's very angry at Hasidic diamond dealers because it turns out that they'll have a cousin in Jerusalem and their cousin in Jerusalem be like, okay, I'm gonna, I'm gonna call Jaim over in New York and he's just gonna do it for me because we're cousins. And so it turns out kinship networks are like an amazing source of social cohesion and, and data transfer. And as that sort of stuff comes apart, as we become more atomized, you have to create tools that control for the atomization. But then that actually tends to create a spiral of, okay, well I don't need to trust the person, so why even bother trust the person. Yeah. I I I think that's all right. I, I tend to think of these things in like different buckets, which is, you know, take just the, the invention of money as a technology to scale this idea of I don't need to trust you because I can, I can pay you money and you'll, you know, give me the service that I need and, and then therefore I don't have to trust you and have this barter system or have a, have a debt based system.


And so I think they, they kinda have to coexist. And the reality is, we exist now in a global world. We exist, you know, in a highly interdependent world. The internet allows us to talk to, to anybody in the world and communicate and learn from anybody in the world. And so the technologies allow us to sort of interact with each other and, and transact with each other without knowing each other. I think are, are great enablers for productivity and learning and, you know, advancement and progress and innovation and all the things that we need as, as at sort of at the species level. I don't think they're a substitute for, for social cohesion. I think it's, it's a separate problem. We need to solve both. Like, I think we need to solve the problem of what does it actually mean to be in a society and be in a community and, and how do we foster those kinds of bonds? Because at the end of of the day, we're still, we're still atoms and we're still humans.


We're still social primates. And, and that's a very important part of society. We also have to solve this problem of, you know, there are now 7 billion people on earth, and we can create tremendous productivity. We can create tremendous growth. We can educate, you know, if you think about, just say from a, from a meritocratic perspective, we're sort of a, a, an opportunity perspective of the 7 billion brains on earth, how many have we really tapped? You know, there's one Elon, there's one Zuck, there's one Bezos. You know, they're, they're not, you know, I, I would argue maybe, maybe we've tapped 7 million people, you know, and, and properly fed them and given them the resources and education and given them a smart, you know, phone in their pocket. And, and really that number should be like, the top 10% of humans are actually pretty smart.


So we should have 700 million people who are of that caliber. And so there's like a hundred x left in humanity if you can get the resources to these people. But to do that, you have to have these tools that can operate at global scale. So you kind of, I think, need both. I think that one is not a substitute for the other. This has been awesome. Really interesting. And where should folks go to check out more of your work On I, on Twitter? Probably just a of on Twitter. And we, we, I publish a lot of thoughts there, but a lot of it frankly is, is in private conversations. Like the really fun stuff has in private. Well, This has been awesome. Really appreciate the time. Great to see you, Ben Shapiro's Sunday special is produced by Ana Morris and Mac Kemp.


Associate producers are Jake Pollock and John Craig. Production coordinator is Jessica Krams, production assistant is Sarah Steele. Editing is by Olivia Stewart. Audio is mixed by Mike Corina. Camera and Lighting is by Zach gta, hair, makeup, and Wardrobe by Fabiola Christina. Title graphics are by Cynthia Angulo, executive assistant Kelly Carvallo. Executive in charge of production is David Waris, executive producer Justin Siegel, executive producer Jeremy Borin. The Ben Shapiro Show. Sunday Special is a daily wire production, copyright daily wire. It's on 24.