
Instacart is on a mission to have you not leave the couch this basketball season because between the pre-game rituals and the post-game interviews, it can be difficult to find time for everything else. So let Instacart take care of your game day snacks or weekly restocks and get delivery in as fast as 30 minutes because we hear it's bad luck to be hungry on game day. So download the Instacart app today and enjoy $0 delivery fees on your first three orders. Service fees apply for three orders in 14 days, excludes restaurants.


Trust isn't just earned, it's demanded. Whether you're a startup founder navigating your first audit or a seasoned security professional scaling your GRC program, proving your commitment to security has never been more critical or more complex. That's where Vanta comes in. Businesses use Vanta to establish trust by automating compliance needs across over 35 frameworks like SOC two and ISO 27 0 0 1. Centralized security workflows, complete questionnaires up to five times faster and proactively manage vendor risk. Vanta not only saves you time, it can also save you money. A new IDC white paper found that Vanta customers achieve $535,000 per year-end benefits and the platform pays for itself in just three months. Join over 9,000 global companies like Atlassian, Quora, and Factory who use Vanta to manage risk and prove security in real time. For a limited time, get $1,000 off vanta@vanta.com slash TED audio. That's VANT a.com/ TED audio for $1,000 off.


Ryan Reynolds here from Mint Mobile. I don't know If you knew this, but anyone can get the same premium wireless for $15 a month plan that I've been enjoying. It's not just for celebrities s Odoo like I did, and have one of your assistants assistants switch you to Mint Mobile today. I'm told it's super easy to do at Mint Mobile dot com slash switch. Upfront payment of $45 for three month plan equivalent to $15 per month required intro rate first three months only. Then full price plan options available, taxes and fees, extra fee, default terms at Mint Mobile dot com.


You're listening to TED Talks Daily where we bring you new ideas to spark your curiosity every day. I'm your host, Elise Hugh. It turns out it's not just humans who think fast and slow. AI models also need time to think if we are wanting them to perform better. In his 2024 talk open AI research scientist, Noam Brown shares new understanding about AI that can inform how to make models work better and do more at scale. It's coming up. The incredible progress in AI over the past five years can be summarized in one word scale.


Yes, there have been algorithmic advances, but the frontier models of today are still based on the same transformer architecture that was introduced in 2017, and they are trained in a very similar way to the models that were trained in 2019. The main difference is the scale of the data and compute that goes into these models. In 2019, GBT two cost about $5,000 to train every year. Since then, for the past five years, the models have gotten bigger, train for longer on more data, and every year they've gotten better.


But today's frontier models can cost hundreds of millions of dollars to train, and there are reasonable concerns among some that AI will soon plateau or hit a wall. After all, are we really going to train models that cost hundreds of billions of dollars? What about trillions of dollars? At some point, the scaling paradigm breaks down. This is, in my opinion, a reasonable concern and in fact it's one that I use to share. But today I am more confident than ever that AI will not plateau. And in fact, I believe that we will see AI progress accelerate in the coming months.


To explain why I wanna tell a story for my time as a PhD student, I started my PhD in 2012 and I was lucky to be able to work on the most exciting projects I could imagine developing ais that could learn on their own how to play poker. Now, I had played a lot of poker when I was in high school and college, so for me, this was basically my childhood dream job. Now, contrary to its reputation, poker is not just a game of luck, it's also a game of deep strategy. You can kind of think of it like chess with a deck of cards. When I started my PhD, there had already been several years of research on how to make ais that play poker.


And the general feeling among the research community is that we had figured out the paradigm and now all we needed to do was scale it. So every year we would train larger poker eyes for longer on more data, and every year they would get better. Just like today's frontier language models. By 2015, they got so good that we thought they might be able to rival the top human experts. So we challenged four of the world's top poker players to an 80,000 hand poker competition with $120,000 in prize money to incentivize them to play their best. And unfortunately, our bot lost by a wide margin.


In fact, it was clear even on day one that our bot was outmatched. But during this competition I noticed something interesting. You see, leading up to this competition, our bot had played almost a trillion hands of poker over thousands of CPUs for about three months. But when it came time to actually play against these human experts, the bot acted instantly. It took about 10 milliseconds to make a decision no matter how difficult it was. It meanwhile, the human experts had only played maybe 10 million hands of poker in their lifetimes. But when they were faced with a difficult decision, they would take the time to think If it was an easy decision, they might only think for a couple seconds.


If it was a difficult decision, they might think for a few minutes, but they would take advantage of the time that they had to think through their decisions. In Daniel Kahneman's book, thinking Fast and Slow, he describes this as the difference between system one thinking and system two thinking. System one thinking is the faster, more intuitive kind of thinking that you might use. For example, to recognize a friendly face or laugh at a funny joke. System two thinking is the slower, more methodical thinking that you might use for things like planning a vacation or writing an essay or solving a hard math problem. After this competition, I wondered whether the system two thinking might be what's missing from a bot.


It might explain the difference in the performance between our bot and the human experts. So I ran some experiments to see just how much of a difference this system two thinking makes in poker and the results that I got blew me away. It turned out that having the bot think for just 20 seconds in a hand of poker got the same boost in performance as scaling up the model by a hundred thousand x and training it for a hundred thousand times longer. Lemme say that again. Spending 20 seconds thinking in a hand of poker got the same boost in performance as scaling up the size of the model and the training by a hundred thousand x.


When I got this result, I literally thought it was a bug. For the first three years of my PhD, I had managed to scale up these models by a hundred x. I was proud of that work. I had written multiple papers on how to do that scaling, but I knew pretty quickly that all of that would be a footnote compared to just scaling up system two thinking. So based on these results, we redesigned the poker AI from the ground up. Now we were focused on scaling up system two thinking in addition to System one. And in 2017 we again challenged four of the world's top poker pros to a 120,000 hand poker competition this time with $200,000 in prize money.


And this time we beat all of them by a huge margin. This was a huge surprise to everybody involved. It was a huge surprise to the poker community. It was a huge surprise to the AI community and honestly even a huge surprise to us. I literally did not think it was possible to win by the kind of margin that we won by. In fact, I think what really highlights just how surprising this result was is that when we announced the competition, the poker community decided to do what they do best and gamble on who would win. When we started, when we announced the competition, the betting odds were about four to one against us. After the first three days of the competition when we had won for the first three days, the betting odds were still about 50 50, but by the eighth day of the competition, you could no longer gamble on which side would win.


You can only gamble on which human would lose the least by the end. This pattern of AI benefiting by thinking for longer is not unique to poker. And in fact, we've seen it in multiple other games as well. For example, in 1997, IBM created Deep Blue an AI that plays chess, and they challenged the world champion Gary Kasparov to a tournament and beat him in a landmark achievement for ai. But deep blue didn't act instantly. Deep blue thought for a couple minutes before making each move. Similarly, in 2016, DeepMind created AlphaGo and he other plays the game of Go, which is even more complicated than the game of chess. And they too challenged a world champion Lisa Al, and beat him in a landmark achievement for ai.


But AlphaGo also didn't act instantly. AlphaGo took the time to think for a couple minutes before making each move. In fact, the authors of AlphaGo later published a paper where they measured just how much of a difference this thinking time makes for the strongest version of AlphaGo. And what they found is that when AlphaGo had the time to think for a couple minutes, it would beat any human alive by a huge margin. But when it had to act instantly, it would do much worse than top humans. In 2021, there was a paper that was published that tried to measure just how much of a difference this thinking time made a bit more scientifically in it.


The authors found that in these games scaling up thinking time by 10 x was roughly the equivalent of scaling up the model size and training by 10 x. So you have this very clear, clean relationship between scaling up system two, thinking time and scaling up system one training. Now why does this matter? Well, remember I mentioned at the start of this talk that today's frontier models cost hundreds of millions of dollars to train, but the cost of querying them, the cost of asking a question and getting an answer is fractions of a penny. So this result says that If you want an even better model, there are two ways you could do it.


One is to keep doing what we've been doing for the past five years and scaling up system one training go from spending hundreds of millions of dollars on a model to billions of dollars on a model. The other is to scale up system two thinking and go from spending a penny per query to 10 cents per query. At a certain point that trade off becomes well worth it. Now of course, all of these results are in the domain of games and there was a reasonable question about whether these results could be extended to a more complicated setting like language. But recently my colleagues and I at OpenAI released oh one a new series of language models that think before responding.


If it's an easy question, oh one might only think for a few seconds if it's a difficult decision, it might think for a few minutes. But just like the ais for chess go and poker oh one benefits, by being able to think for longer, this opens up a completely new dimension for scaling. We're no longer constrained to just scaling up system one training. Now we can scale up system two thinking as well. And the beautiful thing about scaling up in this direction is that it's largely untapped. Remember I mentioned that the frontier models of today cost less than a penny to query. Now, when I mention this to people, a frequent response that I get is that people might not be willing to wait around for a few minutes to get a response from a model or pay a few dollars to get an answer to their question.


And it's true that oh one takes longer and costs more than other models that are out there. But I would argue that for some of the most important problems that we care about, that cost is well worth it. So let's do an experiment and see, raise your hand If you would be willing to pay more than a dollar for a new cancer treatment. Alright, basically everybody in the audience keep your hand up. How about a thousand dollars? How about a million dollars? What about for more efficient solar panels or for a proof of the reman hypothesis? The common conception of AI today is chatbots, but it doesn't have to be that way.


This isn't a revolution that's 10 years away or even two years away. It's a revolution that's happening now. My colleagues and I have already released a one preview and I have had people come to me and say that it has saved them days worth of work, including researchers at top universities. And that's just the preview I mentioned at the start of this talk that the history of AI progress over the past five years can be summarized in one word scale. So far that has meant scaling up the system one training of these models. Now we have a new paradigm, one where we can scale up system two thinking as well. And we are just at the very beginning of scaling up in this direction.


Now I know that there are some people who will still say that AI is going to plateau or hit a wall, and to them I say wanna bet. Thank you. That was Noam Brown at TED AI San Francisco in 2024. If you're curious about Ted's curation, find out more at TED dot com slash curation guidelines. And that's it for today. TED Talks Daily is part of the TED Audio Collective. This episode was produced and edited by our team, Martha Esnos, Oliver Friedman, Brian Green, autumn Thompson, and Alejandra Salazar.


It was mixed by Christopher Fay Bogan. Additional support from Emma Toner and Daniella Bazo. I'm Elise Hugh. I'll be back tomorrow with a fresh idea for your feet. Thanks for listening.


Race the rudder brace, the Sail Race, the sales captain. An unidentified ship is approaching over Roger, wait. Is that an enterprise sales solution? Reach sales professionals, not professional sailors. With LinkedIn ads, you can target the right people by industry job title and more. We'll even give you a $100 credit on your next campaign. Get started today at LinkedIn dot com slash results. Terms and conditions apply.


And now two pigeons. Be moaning. The fact you can choose the TV you want with Direct tv. So now they can pick out their favorite entertainment when they sign up and save money. Well, la da you mean If you like sports, you can get a sports pack. What about entertainment? Entertainment Pack? It's like finding an old pizza with only your favorite toppings, which for me would be Pineapple and toenails. Ew, pineapple Pay for the TV you want. Not the TV you don't. Visit DirecTV stream.com. Service renews monthly at end prevailing rate unless you cancel online. New eligible customers only restrictions apply.


Hi, I'm Maya from Girls At Invest Tax season is all about preparation. And if you're like most people managing your financial logins and documents, securely can feel overwhelming. One password can help bring order to the tax prep chaos all in one place. Right now you get 25% off your first year of one password individual or 50% off your first year of one Password families at one password.com/girls that invest. That's 25 or 50% off your first year at one PASS w.com/girls that invest or lowercase one password.com/girls that invest.