{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abeb2502-84eb-496f-b9db-c1ce1397ec15",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "This notebook contains the code used for topic modeling our podcast and news corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9d1a0-f176-4a55-aac9-32a971663576",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Add necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f23a7f-3b4c-48be-9ecc-688c967154cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic # topic modeling\n",
    "from sentence_transformers import SentenceTransformer # embeddings\n",
    "import os # file haldling\n",
    "import pandas as pd # data exploration\n",
    "import math # for chunking function\n",
    "import pickle # for loading articles\n",
    "import random # for sampling our data\n",
    "import numpy as np # for exporting embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d46abb-129b-4010-9ffd-1c2368e280cb",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Here, we preprocess our data for topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629874db-3460-4fca-9c89-555759fad049",
   "metadata": {},
   "source": [
    "### Transcripts\n",
    "Transcripts are already clean, with no timestamps, speaker tags, or non-speech annotations. BERTopic also does not require stemming, lemmatizing, or stopword removal. \n",
    "\n",
    "Preprocessing is thus limited to removing those transcripts for podcasts published by traditional news outlets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b6722-b51b-4aa5-a385-5da56d3e71cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_df = pd.read_csv(\"data/top_50_pods_USA_2025_Q1.csv\", index_col=\"Rank\")\n",
    "\n",
    "# array of podcast titles to include\n",
    "to_include = podcast_df[~podcast_df[\"Exclude\"]][\"Title\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff162914-3ca8-42a7-a96b-39f9765e5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_table = pd.read_csv(\"data/podcasts/all_podcasts_metadata.csv\")\n",
    "\n",
    "# removing all podcasts not on this list\n",
    "metadata_table = metadata_table[metadata_table[\"podcast_name\"].isin(to_include)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e1fe4-2f1b-42b0-991d-ffa718f8a8b7",
   "metadata": {},
   "source": [
    "### News Articles\n",
    "News articles do not require proprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1302e-0bc4-4f71-88ee-2cd692058781",
   "metadata": {},
   "source": [
    "## Text Chunking\n",
    "\n",
    "As podcasts and news articles can be long and often contain multiple topics, we break these into shorter documents so that our model will have an easier time grouping similar ideas together. Chunking also gives us even more datapoints, with more evidence for each topic and accordingly more robust topic discovery.\n",
    "\n",
    "For our embeddings, we default to a `chunk size` of 300, which tends to be the \"sweet spot\" (i.e. long enough to be meaningful, short enough to not mix topics too much). We can adjust this based on modeling results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ddc57a-09ba-4e2b-9667-e14c7e747871",
   "metadata": {},
   "source": [
    "### General Chunking Function\n",
    "Note, this function is designed to make chunks as even as possible without a potential tiny chunk at the end of the transcript for reasons of consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7768d0-4219-42e5-bcc1-f39a10580f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=300):\n",
    "    \"\"\"Split documents into even chunks of approximately chunk_size\"\"\"\n",
    "    words = text.split()\n",
    "    n_chunks = math.ceil(len(words) / chunk_size)\n",
    "    if n_chunks == 0:\n",
    "        return []\n",
    "    avg = len(words) // n_chunks\n",
    "    rem = len(words) % n_chunks\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for i in range(n_chunks):\n",
    "        # distribute remainder if any to the first chunk\n",
    "        extra = 1 if i < rem else 0 \n",
    "        end = start + avg + extra\n",
    "        chunks.append(' '.join(words[start:end]))\n",
    "        start = end\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7aba3-bb06-435c-8692-92f703a715d9",
   "metadata": {},
   "source": [
    "### Chunking Transcripts\n",
    "Here we apply chunking to our podcast transcripts. Our outputs are:\n",
    "\n",
    "1. `pod_docs`: a list containing chunks from all transcripts.\n",
    "2. `pod_chunk_metadata`: a list of dictionaries containing for each chunk, its source type, source name, title, publish date, filename and unique id (within the document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d136beed-89e2-4a6c-b5bc-7ba6e0a5137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_docs = []\n",
    "pod_chunk_metadata = []\n",
    "\n",
    "for idx, row in metadata_table.iterrows():\n",
    "    filepath = row[\"filename\"]\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            chunks = chunk_text(text, chunk_size=300)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                pod_docs.append(chunk)\n",
    "                # Attach all the original metadata and give each chunk a unique id for tracking\n",
    "                pod_chunk_metadata.append({\n",
    "                    \"filename\": row[\"filename\"],\n",
    "                    \"source_type\": \"podcast\",\n",
    "                    \"source_name\": row[\"podcast_name\"],\n",
    "                    \"title\": row[\"episode_title\"],\n",
    "                    \"publish_date\": row[\"publish_date\"],\n",
    "                    \"chunk_id\": i\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ea747-b0bc-45e6-be7a-06feef5037c8",
   "metadata": {},
   "source": [
    "### Chunking News Articles\n",
    "Here, we do the same chunking on our news articles. \n",
    "\n",
    "First, we load our news articles, which have been exported from Pandas DataFrames into `.pkl` files. We aggregate these into `news_articles`, a list of dictionaries where each article's text content is paired with keys for `outlet_name`, `title`, `date`, `filename`.\n",
    "\n",
    "Then, we perform the same chunking we did on our podcast transcripts. Our outputs are:\n",
    "\n",
    "1. `news_docs`: a list containing chunks from all news articles.\n",
    "2. `news_chunk_metadata`: a list of dictionaries containing for each chunk, its source type, source name, title, publish date, filename and unique id (within the document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42d9b5-79dc-4c4b-b5e8-f9c6b80763ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_filepaths():\n",
    "    found_files = []\n",
    "    for root, _, files in os.walk(\"data/article data\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pkl\") and \"combined\" in file:\n",
    "                found_files.append(os.path.join(root, file))\n",
    "    return found_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7fe3b0-42d7-4f4f-81d2-ccb8a15954da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_news_articles(filepaths):\n",
    "    articles = []\n",
    "    for fp in filepaths:\n",
    "        outlet_name = \"NPR\" if \"NPR\" in fp else \"PBS\" if \"PBS\" in fp else \"Unknown\"\n",
    "        df = pd.read_pickle(fp)\n",
    "        for idx, row in df.iterrows():\n",
    "            if row[\"content\"]:\n",
    "                articles.append({\n",
    "                    \"outlet_name\": outlet_name,\n",
    "                    \"title\": row[\"title\"],\n",
    "                    \"date\": row.get(\"date\", None),\n",
    "                    \"content\": \" \".join(row[\"content\"]) if isinstance(row[\"content\"], list) else row[\"content\"],  # join content if it's a list\n",
    "                    \"filename\": fp\n",
    "                })\n",
    "    return articles\n",
    "\n",
    "news_filepaths = get_article_filepaths()\n",
    "news_articles = load_news_articles(news_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02147a83-3fec-499e-9a53-573708f11992",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_docs = []\n",
    "news_chunk_metadata = []\n",
    "\n",
    "for article in news_articles:\n",
    "    chunks = chunk_text(article[\"content\"], chunk_size=300)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        news_docs.append(chunk)\n",
    "        news_chunk_metadata.append({\n",
    "            \"source_type\": \"news\",\n",
    "            \"source_name\": article[\"outlet_name\"],\n",
    "            \"title\": article[\"title\"],\n",
    "            \"publish_date\": article[\"date\"],\n",
    "            \"filename\": article[\"filename\"],\n",
    "            \"chunk_id\": i\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a7207-623a-490e-a9e0-9256d4d24ead",
   "metadata": {},
   "source": [
    "## Running Topic Model\n",
    "Here we run our topic model on our combined news and podcast corpus. For the sake of time, below is a run on 10,000 randomly sampled chunks (approximately 2% of our total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bad3d-f16f-422c-aceb-067b559e8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = pod_docs + news_docs # joining our document corpora\n",
    "all_chunk_metadata = pod_chunk_metadata + news_chunk_metadata # joining our metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d07946-4c82-45e4-912f-f3e9d492b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = random.sample(range(len(all_docs)), 10000) # taking a random sample of 10,000 chunks\n",
    "\n",
    "sampled_docs = [all_docs[i] for i in sample_indices]\n",
    "sampled_metadata = [all_chunk_metadata[i] for i in sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c90a6-ac84-4fea-b0c6-39566665d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = embedding_model.encode(all_docs, show_progress_bar=True) # replace with all_docs for full run\n",
    "topic_model = BERTopic(embedding_model=embedding_model, verbose=True)\n",
    "topics, probs = topic_model.fit_transform(all_docs, embeddings) # replace with all_docs for full run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf9ab1-68c6-4f98-8837-b2833b220229",
   "metadata": {},
   "source": [
    "## Exploring Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04635c-8cab-4b4f-9691-07d5bca6247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe containing topic ID, number of chunks assigned to the topic, and topic keywords\n",
    "topic_model.get_topic_info().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4866044-8b07-40c7-8989-d0c8c8be1352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `results` DataFrame contains the our chunk metadata along with their most likely topic.\n",
    "results = pd.DataFrame(all_metadata) # replace with all_chunk_metadata for full run\n",
    "results[\"text\"] = all_docs # replace with all_docs for full run\n",
    "results[\"topic\"] = topics\n",
    "results[\"probability\"] = probs\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7130db9-7087-4ee9-98a8-888baebfbf44",
   "metadata": {},
   "source": [
    "## Exporting Model and Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0982f-715f-4db0-9aee-52c59bbfb449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model\n",
    "topic_model.save(\"models/bertopic_model_full\")\n",
    "\n",
    "# Export our embeddings\n",
    "np.save(\"sampled_docs_embeddings.npy\", embeddings)\n",
    "\n",
    "# Export our results\n",
    "results.to_csv(\"data/bertopic_results.csv\", index=False)\n",
    "topic_model.get_topic_info().to_csv(\"data/topic_info.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
