{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e8e063-7739-435a-85df-95358afc763e",
   "metadata": {},
   "source": [
    "# Framing Insights\n",
    "This notebook extracts key insights from our framing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ed93b-4160-44b7-a08b-8bf8e88ca5f8",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Add necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e016f-a709-4ab6-a0e5-ed63e7637583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, ast\n",
    "from typing import Literal, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d920c8-1500-4e5d-bd1a-c0099c640a3f",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bc3c1-b7fc-416b-8f04-498a5c8f3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords_df = pd.read_parquet(\"data/topwords_by_topic.parquet\")\n",
    "avg_sentiment = pd.read_csv(\"data/avg_sentiment_by_source_topic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8569e-8734-4b52-976b-4a41dad52ac5",
   "metadata": {},
   "source": [
    "## Exploratory Insights\n",
    "Here, we look at cross-source contrasts for podcasts vs news. We compute a number of similarity metrics, comparing both sentiment and words used. Examplars and topic-level framing sights are displayed as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd744e-e2a0-4836-ad35-ca94edfd4baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _json_loads_deep(x, max_depth=2):\n",
    "    val = x\n",
    "    for _ in range(max_depth):\n",
    "        if not isinstance(val, str):\n",
    "            break\n",
    "        try:\n",
    "            val = json.loads(val)\n",
    "        except Exception:\n",
    "            break\n",
    "    return val\n",
    "\n",
    "# helper function to parse objects into python lists for analysis\n",
    "def _parse_list_like(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, (np.ndarray, pd.Series)):\n",
    "        try:\n",
    "            return list(x.tolist())\n",
    "        except Exception:\n",
    "            try:\n",
    "                return list(x)\n",
    "            except Exception:\n",
    "                return []\n",
    "    if isinstance(x, (dict, tuple, set)):\n",
    "        try:\n",
    "            return list(x)\n",
    "        except Exception:\n",
    "            return []\n",
    "    if isinstance(x, str):\n",
    "        val = _json_loads_deep(x, max_depth=2)\n",
    "        if isinstance(val, list):\n",
    "            return val\n",
    "        try:\n",
    "            val2 = ast.literal_eval(x)\n",
    "            if isinstance(val2, list):\n",
    "                return val2\n",
    "        except Exception:\n",
    "            pass\n",
    "    return []\n",
    "\n",
    "def words_to_series_weighted(\n",
    "    top_words,\n",
    "    key_text=(\"text\",\"token\",\"word\",\"term\"),\n",
    "    key_val=(\"value\",\"score\",\"weight\")\n",
    "):\n",
    "    lst = _parse_list_like(top_words)\n",
    "    pairs = {}\n",
    "    for d in lst:\n",
    "        if not isinstance(d, dict):\n",
    "            continue\n",
    "        w = None\n",
    "        for kt in key_text:\n",
    "            if kt in d and isinstance(d[kt], str):\n",
    "                w = d[kt].strip()\n",
    "                break\n",
    "        v = None\n",
    "        for kv in key_val:\n",
    "            if kv in d:\n",
    "                try:\n",
    "                    v = float(d[kv])\n",
    "                except Exception:\n",
    "                    v = None\n",
    "                break\n",
    "        if w and (v is not None) and np.isfinite(v) and v > 0:\n",
    "            pairs[w] = v\n",
    "    return pd.Series(pairs, dtype=float)\n",
    "\n",
    "# turns words into a series in case of a string object\n",
    "def words_to_series_plain(top_words_plain):\n",
    "    lst = _parse_list_like(top_words_plain)\n",
    "    tokens = []\n",
    "    if lst:\n",
    "        tokens = [str(t).strip() for t in lst if isinstance(t, (str, int, float))]\n",
    "    elif isinstance(top_words_plain, str):\n",
    "        s = top_words_plain.strip()\n",
    "        if \",\" in s:\n",
    "            tokens = [t.strip() for t in s.split(\",\")]\n",
    "        else:\n",
    "            tokens = s.split()\n",
    "    tokens = [t for t in tokens if t]\n",
    "    return pd.Series({t: 1.0 for t in tokens}, dtype=float)\n",
    "\n",
    "# various measures of jaccard similarity\n",
    "def jaccard(a_set, b_set):\n",
    "    inter = len(a_set & b_set)\n",
    "    uni = len(a_set | b_set)\n",
    "    return inter / uni if uni else np.nan\n",
    "\n",
    "def weighted_jaccard(a: pd.Series, b: pd.Series):\n",
    "    idx = a.index.union(b.index)\n",
    "    a_ = a.reindex(idx, fill_value=0.0)\n",
    "    b_ = b.reindex(idx, fill_value=0.0)\n",
    "    num = float(np.minimum(a_, b_).sum())\n",
    "    den = float(np.maximum(a_, b_).sum())\n",
    "    return float(num / den) if den > 0 else np.nan\n",
    "\n",
    "def _ensure_unique(df, by, agg_map=None):\n",
    "    if df.duplicated(by).any():\n",
    "        if agg_map is None:\n",
    "            agg_map = {}\n",
    "            for c in df.columns:\n",
    "                if c in by:\n",
    "                    continue\n",
    "                agg_map[c] = \"mean\" if pd.api.types.is_numeric_dtype(df[c]) else \"first\"\n",
    "        return df.groupby(by, as_index=False).agg(agg_map)\n",
    "    return df\n",
    "\n",
    "def build_pair_contrasts(\n",
    "    avg_sentiment: pd.DataFrame,\n",
    "    top_words_df: pd.DataFrame,\n",
    "    sourceA: str,\n",
    "    sourceB: str,\n",
    "    id_col: Literal[\"source_name\",\"source_type\"] = \"source_name\",\n",
    "    topic_col: str = \"topic\",\n",
    "    sentiment_col: str = \"avg_sentiment_score\",\n",
    "    q_col: Optional[str] = \"quantile_sentiment_scaled\",\n",
    "    label_col: Optional[str] = \"sentiment_label\",\n",
    "    min_top_words: int = 3,\n",
    "    big_split_abs_delta: float = 0.10,\n",
    "    low_overlap_jaccard: float = 0.20,\n",
    "    high_overlap_jaccard: float = 0.60,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    S = avg_sentiment.copy()\n",
    "    T = top_words_df.copy()\n",
    "    for df in (S, T):\n",
    "        df[topic_col] = df[topic_col].astype(str).str.strip()\n",
    "\n",
    "    id_topic = [id_col, topic_col]\n",
    "    S = _ensure_unique(S, by=id_topic)\n",
    "    T = _ensure_unique(T, by=id_topic)\n",
    "\n",
    "    keep_sent = [topic_col, sentiment_col]\n",
    "    if q_col in S.columns: keep_sent.append(q_col)\n",
    "    if label_col in S.columns: keep_sent.append(label_col)\n",
    "\n",
    "    SA = S.loc[S[id_col] == sourceA, keep_sent].rename(columns={sentiment_col:\"sentA\", q_col:\"qA\", label_col:\"labelA\"})\n",
    "    SB = S.loc[S[id_col] == sourceB, keep_sent].rename(columns={sentiment_col:\"sentB\", q_col:\"qB\", label_col:\"labelB\"})\n",
    "\n",
    "    base = SA.merge(SB, on=topic_col, how=\"inner\")\n",
    "    if verbose:\n",
    "        print(f\"Topics with sentiment A={SA[topic_col].nunique()}, B={SB[topic_col].nunique()}, overlap={base[topic_col].nunique()}\")\n",
    "\n",
    "    if base.empty:\n",
    "        cols = [\"topic\",\"sentA\",\"qA\",\"labelA\",\"sentB\",\"qB\",\"labelB\",\"delta\",\"jaccard\",\"w_jaccard\",\"cosine\",\"n_topA\",\"n_topB\",\"twA\",\"twB\",\"bucket\"]\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    TwA = T.loc[T[id_col] == sourceA, [topic_col, \"top_words\",\"top_words_plain\"]].rename(columns={\"top_words\":\"twA\", \"top_words_plain\":\"twA_plain\"})\n",
    "    TwB = T.loc[T[id_col] == sourceB, [topic_col, \"top_words\",\"top_words_plain\"]].rename(columns={\"top_words\":\"twB\", \"top_words_plain\":\"twB_plain\"})\n",
    "\n",
    "    df = (base.merge(TwA, on=topic_col, how=\"left\")\n",
    "               .merge(TwB, on=topic_col, how=\"left\"))\n",
    "\n",
    "    deltas, jacs, wjacs, coss, nA_list, nB_list = [], [], [], [], [], []\n",
    "    for _, r in df.iterrows():\n",
    "        sA = float(r[\"sentA\"]); sB = float(r[\"sentB\"])\n",
    "        deltas.append(sA - sB)\n",
    "\n",
    "        serA = words_to_series_weighted(r.get(\"twA\"))\n",
    "        serB = words_to_series_weighted(r.get(\"twB\"))\n",
    "\n",
    "        if serA.size == 0:\n",
    "            serA = words_to_series_plain(r.get(\"twA_plain\"))\n",
    "        if serB.size == 0:\n",
    "            serB = words_to_series_plain(r.get(\"twB_plain\"))\n",
    "\n",
    "        nA, nB = int(serA.size), int(serB.size)\n",
    "\n",
    "        nA_list.append(nA); nB_list.append(nB)\n",
    "\n",
    "        if (nA >= min_top_words) and (nB >= min_top_words):\n",
    "            j = jaccard(set(serA.index), set(serB.index))\n",
    "            wj = weighted_jaccard(serA, serB)\n",
    "            cs = cosine_sim(serA, serB)\n",
    "        else:\n",
    "            j = wj = cs = np.nan\n",
    "\n",
    "        jacs.append(j); wjacs.append(wj); coss.append(cs)\n",
    "\n",
    "    df[\"delta\"] = deltas\n",
    "    df[\"jaccard\"] = jacs\n",
    "    df[\"w_jaccard\"] = wjacs\n",
    "    df[\"cosine\"] = coss\n",
    "    df[\"n_topA\"] = nA_list\n",
    "    df[\"n_topB\"] = nB_list\n",
    "\n",
    "    absÎ” = df[\"delta\"].abs()\n",
    "    no_tw = df[\"jaccard\"].isna()\n",
    "    df[\"bucket\"] = np.select(\n",
    "        [\n",
    "            (absÎ” >= big_split_abs_delta) & (~no_tw) & (df[\"jaccard\"] <= low_overlap_jaccard),\n",
    "            (absÎ” <= 0.05) & (~no_tw) & (df[\"jaccard\"] <= low_overlap_jaccard),\n",
    "            (absÎ” >= big_split_abs_delta) & (~no_tw) & (df[\"jaccard\"] >= high_overlap_jaccard),\n",
    "            (absÎ” >= big_split_abs_delta) & (no_tw),\n",
    "        ],\n",
    "        [\n",
    "            \"big_split_diff_frames\", \"same_sent_diff_frames\", \"big_split_same_frames\", \"big_split_no_topwords\",\n",
    "        ],\n",
    "        default=\"other\"\n",
    "    )\n",
    "\n",
    "    order_cols = [\"topic\",\"sentA\",\"qA\",\"labelA\",\"sentB\",\"qB\",\"labelB\",\"delta\",\n",
    "                  \"jaccard\",\"w_jaccard\",\"cosine\",\"n_topA\",\"n_topB\",\"twA\",\"twB\",\"bucket\"]\n",
    "    return df.sort_values(by=\"delta\", key=lambda s: s.abs(), ascending=False)[order_cols].reset_index(drop=True)\n",
    "\n",
    "def parse_words_any(x, x_plain):\n",
    "    s = words_to_series_weighted(x)\n",
    "    if s.size == 0:\n",
    "        s = words_to_series_plain(x_plain)\n",
    "    if s.size > 0 and s.sum() > 0:\n",
    "        s = s / s.sum()\n",
    "    return s\n",
    "\n",
    "def macro_summary(pair: pd.DataFrame, min_top_words=3):\n",
    "    if pair.empty:\n",
    "        return {}\n",
    "    mask_tw = (pair[\"n_topA\"] >= min_top_words) & (pair[\"n_topB\"] >= min_top_words)\n",
    "    with_tw = pair.loc[mask_tw]\n",
    "    out = {\n",
    "        \"n_topics\": int(len(pair)),\n",
    "        \"n_with_topwords_both\": int(mask_tw.sum()),\n",
    "        \"median_abs_delta\": float(pair[\"delta\"].abs().median()),\n",
    "        \"mean_abs_delta\": float(pair[\"delta\"].abs().mean()),\n",
    "        \"share_A_more_positive\": float((pair[\"delta\"] > 0).mean()),\n",
    "        \"share_B_more_positive\": float((pair[\"delta\"] < 0).mean()),\n",
    "        \"median_jaccard_with_tw\": float(with_tw[\"jaccard\"].median()) if not with_tw.empty else np.nan,\n",
    "        \"bucket_counts\": pair[\"bucket\"].value_counts().to_dict(),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def contrast_terms(row, top=5):\n",
    "    sA = parse_words_any(row[\"twA\"], row.get(\"twA_plain\", None))\n",
    "    sB = parse_words_any(row[\"twB\"], row.get(\"twB_plain\", None))\n",
    "\n",
    "    idxA, idxB = set(sA.index), set(sB.index)\n",
    "    common = list(idxA & idxB)\n",
    "    onlyA  = list(idxA - idxB)\n",
    "    onlyB  = list(idxB - idxA)\n",
    "\n",
    "    common_sorted = sorted(common, key=lambda w: min(sA.get(w,0), sB.get(w,0)), reverse=True)[:top]\n",
    "    uniqA_sorted = list(sA.sort_values(ascending=False).loc[onlyA].head(top).index) if onlyA else []\n",
    "    uniqB_sorted = list(sB.sort_values(ascending=False).loc[onlyB].head(top).index) if onlyB else []\n",
    "\n",
    "    union = sA.index.union(sB.index)\n",
    "    diff = pd.Series({w: sA.get(w,0) - sB.get(w,0) for w in union})\n",
    "    tiltA = list(diff.sort_values(ascending=False).head(top).index)\n",
    "    tiltB = list(diff.sort_values(ascending=True).head(top).index)\n",
    "\n",
    "    return {\n",
    "        \"overlap_words\": \", \".join(common_sorted),\n",
    "        \"distinct_A\": \", \".join(uniqA_sorted),\n",
    "        \"distinct_B\": \", \".join(uniqB_sorted),\n",
    "        \"tilt_A\": \", \".join(tiltA),\n",
    "        \"tilt_B\": \", \".join(tiltB),\n",
    "    }\n",
    "\n",
    "def exemplars(pair_df: pd.DataFrame, bucket: str, k=5, min_top_words=3):\n",
    "    m = (pair_df[\"bucket\"] == bucket) & (pair_df[\"n_topA\"] >= min_top_words) & (pair_df[\"n_topB\"] >= min_top_words)\n",
    "    sub = pair_df.loc[m].copy()\n",
    "    if sub.empty:\n",
    "        return pd.DataFrame(columns=[\"topic\",\"delta\",\"sentA\",\"sentB\",\"overlap_words\",\"distinct_A\",\"distinct_B\",\"tilt_A\",\"tilt_B\"])\n",
    "    rows = []\n",
    "    for _, r in sub.sort_values(by=\"delta\", key=lambda s: s.abs(), ascending=False).head(k).iterrows():\n",
    "        terms = contrast_terms(r, top=4)\n",
    "        rows.append({\n",
    "            \"topic\": r[\"topic\"],\n",
    "            \"delta\": round(float(r[\"delta\"]), 3),\n",
    "            \"sentA\": round(float(r[\"sentA\"]), 3),\n",
    "            \"sentB\": round(float(r[\"sentB\"]), 3),\n",
    "            **terms\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def exemplar_text_rows(df: pd.DataFrame, sideA_name=\"A\", sideB_name=\"B\"):\n",
    "    lines = []\n",
    "    for _, r in df.iterrows():\n",
    "        line = (\n",
    "            f\"{r['topic']} (Î”={r['delta']:+.3f}; {sideA_name}={r['sentA']:+.3f}, {sideB_name}={r['sentB']:+.3f}) \"\n",
    "            f\"Overlap: [{r['overlap_words']}]. \"\n",
    "            f\"{sideA_name}-distinct: [{r['distinct_A']}]; \"\n",
    "            f\"{sideB_name}-distinct: [{r['distinct_B']}].\"\n",
    "        )\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "def find_topics(pair_df, query=\"trump\"):\n",
    "    q = query.lower()\n",
    "    return sorted([t for t in pair_df[\"topic\"].unique() if q in t.lower()])\n",
    "\n",
    "def topic_view(pair_df, topic, sideA=\"Podcasts\", sideB=\"News\", top=6):\n",
    "    row = pair_df.loc[pair_df[\"topic\"]==topic].iloc[0]\n",
    "    a = words_to_series_weighted(row.get(\"twA\"));  b = words_to_series_weighted(row.get(\"twB\"))\n",
    "    a = clean_token_series(a, stoplist); b = clean_token_series(b, stoplist)\n",
    "    a = a/a.sum() if a.sum()>0 else a\n",
    "    b = b/b.sum() if b.sum()>0 else b\n",
    "    common = list(set(a.index) & set(b.index))\n",
    "    common = sorted(common, key=lambda w: min(a.get(w,0), b.get(w,0)), reverse=True)[:top]\n",
    "    tiltA = (a - b.reindex(a.index, fill_value=0)).sort_values(ascending=False).head(top).index.tolist()\n",
    "    tiltB = (b - a.reindex(a.index, fill_value=0)).sort_values(ascending=False).head(top).index.tolist()\n",
    "    return {\n",
    "        \"topic\": topic,\n",
    "        \"delta\": float(row[\"delta\"]),\n",
    "        sideA+\"_sent\": float(row[\"sentA\"]),\n",
    "        sideB+\"_sent\": float(row[\"sentB\"]),\n",
    "        \"overlap\": common,\n",
    "        sideA+\"_tilt\": tiltA,\n",
    "        sideB+\"_tilt\": tiltB,\n",
    "        \"jaccard_clean\": float(row.get(\"jaccard_clean\", np.nan)),\n",
    "    }\n",
    "\n",
    "def npr_similarity(avg_sentiment, topwords_df, podcasts, baseline=\"NPR\", min_top_words=5):\n",
    "    rows = []\n",
    "    for p in podcasts:\n",
    "        pair = build_pair_contrasts(avg_sentiment, topwords_df, baseline, p, id_col=\"source_name\", verbose=False)\n",
    "        if pair.empty:\n",
    "            continue\n",
    "        stat = global_token_stats(topwords_df, id_col=\"source_name\")\n",
    "        stoplist_names = make_stoplist(stat, min_total_topics=5, min_podcast_share=0.85)\n",
    "        pairC = add_cleaned_overlap(pair, stoplist_names, min_top_words=min_top_words)\n",
    "        corr = np.corrcoef(pairC[\"sentA\"], pairC[\"sentB\"])[0,1] if pairC[\"sentA\"].std()>0 and pairC[\"sentB\"].std()>0 else np.nan\n",
    "        rows.append({\n",
    "            \"podcast\": p,\n",
    "            \"n_topics\": int(len(pairC)),\n",
    "            \"median_abs_delta\": float(pairC[\"delta\"].abs().median()),\n",
    "            \"sent_corr\": float(corr),\n",
    "            \"median_jaccard_clean\": float(pairC[\"jaccard_clean\"].median(skipna=True)),\n",
    "        })\n",
    "    out = pd.DataFrame(rows)\n",
    "    return out.sort_values(by=[\"sent_corr\",\"median_jaccard_clean\",\"median_abs_delta\"], ascending=[False, False, True])\n",
    "\n",
    "def compare_against(avg_sentiment, topwords_df, base_source, others, id_col=\"source_name\", min_top_words=3):\n",
    "    rows = []\n",
    "    for other in others:\n",
    "        pair = build_pair_contrasts(avg_sentiment, topwords_df, base_source, other, id_col=id_col, verbose=False)\n",
    "        if pair.empty:\n",
    "            continue\n",
    "        summ = macro_summary(pair, min_top_words=min_top_words)\n",
    "        rows.append({\n",
    "            \"A\": base_source, \"B\": other,\n",
    "            \"n_topics\": summ.get(\"n_topics\", 0),\n",
    "            \"n_with_topwords_both\": summ.get(\"n_with_topwords_both\", 0),\n",
    "            \"median_abs_delta\": summ.get(\"median_abs_delta\", np.nan),\n",
    "            \"share_A_more_positive\": summ.get(\"share_A_more_positive\", np.nan),\n",
    "            \"share_big_split_diff_frames\": (pair[\"bucket\"]==\"big_split_diff_frames\").mean() if not pair.empty else np.nan,\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(\"median_abs_delta\", ascending=False)\n",
    "\n",
    "def aggregate_distinct_tokens(pair_df, subset_mask, stoplist, side=\"A\", top=30):\n",
    "    pool = []\n",
    "    for _, r in pair_df.loc[subset_mask].iterrows():\n",
    "        s = words_to_series_weighted(r[\"twA\"] if side==\"A\" else r[\"twB\"])\n",
    "        if s.size == 0:\n",
    "            s = words_to_series_plain(r[\"twA_plain\"] if side==\"A\" else r[\"twB_plain\"])\n",
    "        s = clean_token_series(s, stoplist)\n",
    "        s = s / s.sum() if s.sum()>0 else s\n",
    "        pool.append(s)\n",
    "    if not pool:\n",
    "        return pd.Series([], dtype=float)\n",
    "    agg = pd.concat(pool, axis=1).fillna(0).mean(axis=1)\n",
    "    return agg.sort_values(ascending=False).head(top)\n",
    "\n",
    "def make_stoplist(stat, min_total_topics=10, min_podcast_share=0.9, max_len=3, extra_regex=r\"^\\d|\\.com$\"):\n",
    "    stop = set()\n",
    "    for _, r in stat.iterrows():\n",
    "        t = r[\"tokens\"]\n",
    "        if (len(t) <= max_len) or re.search(extra_regex, t):\n",
    "            stop.add(t)\n",
    "        elif (r[\"total_topics\"] >= min_total_topics) and (r[\"podcast_share_topics\"] >= min_podcast_share):\n",
    "            stop.add(t)\n",
    "    brand_seeds = {\"squarespace\",\"bluechew\",\"quince\",\"promo\",\"code\",\"sponsor\",\"midas\",\"com\",\"dot\",\"youtube\",\"subscribe\"}\n",
    "    stop |= brand_seeds\n",
    "    return stop\n",
    "\n",
    "def clean_token_series(s: pd.Series, stoplist: set):\n",
    "    def keep(tok):\n",
    "        t = tok.lower()\n",
    "        if t in stoplist: return False\n",
    "        if len(t) < 3: return False\n",
    "        if any(ch.isdigit() for ch in t): return False\n",
    "        if re.fullmatch(r\"[a-z\\-]+\", t) is None: return False\n",
    "        return True\n",
    "    s2 = s.copy()\n",
    "    s2.index = [w.lower() for w in s2.index]\n",
    "    s2 = s2[s2.index.map(keep)]\n",
    "    return s2\n",
    "\n",
    "def add_cleaned_overlap(pair_df: pd.DataFrame, stoplist: set, min_top_words=3):\n",
    "    j, wj, cs, nA, nB = [], [], [], [], []\n",
    "    for _, r in pair_df.iterrows():\n",
    "        a = words_to_series_weighted(r.get(\"twA\")); b = words_to_series_weighted(r.get(\"twB\"))\n",
    "        if a.size == 0:\n",
    "            a = words_to_series_plain(r.get(\"twA_plain\", None))\n",
    "        if b.size == 0:\n",
    "            b = words_to_series_plain(r.get(\"twB_plain\", None))\n",
    "        a = clean_token_series(a, stoplist); b = clean_token_series(b, stoplist)\n",
    "        a = a / a.sum() if a.sum()>0 else a\n",
    "        b = b / b.sum() if b.sum()>0 else b\n",
    "        a_size, b_size = int(a.size), int(b.size)\n",
    "        nA.append(a_size); nB.append(b_size)\n",
    "        if a.size >= min_top_words and b.size >= min_top_words:\n",
    "            j.append(jaccard(set(a.index), set(b.index)))\n",
    "            wj.append(weighted_jaccard(a, b))\n",
    "            cs.append(cosine_sim(a, b))\n",
    "        else:\n",
    "            j.append(np.nan); wj.append(np.nan); cs.append(np.nan)\n",
    "    out = pair_df.copy()\n",
    "    out[\"jaccard_clean\"] = j; out[\"w_jaccard_clean\"] = wj; out[\"cosine_clean\"] = cs\n",
    "    out[\"n_topA_clean\"] = nA; out[\"n_topB_clean\"] = nB\n",
    "    return out\n",
    "\n",
    "def extremes_by_source(S, id_col=\"source_name\", topic_col=\"topic\", sentiment_col=\"avg_sentiment_score\", k=10):\n",
    "    out = {}\n",
    "    for src, g in S.groupby(id_col):\n",
    "        g2 = g[[topic_col, sentiment_col]].dropna()\n",
    "        out[src] = {\n",
    "            \"most_positive\": g2.sort_values(sentiment_col, ascending=False).head(k).values.tolist(),\n",
    "            \"most_negative\": g2.sort_values(sentiment_col, ascending=True).head(k).values.tolist(),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "def find_topics(pair_df, query=\"trump\"):\n",
    "    q = query.lower()\n",
    "    return sorted([t for t in pair_df[\"topic\"].unique() if q in t.lower()])\n",
    "\n",
    "def topic_view(pair_df, topic, sideA=\"Podcasts\", sideB=\"News\", top=6):\n",
    "    row = pair_df.loc[pair_df[\"topic\"]==topic].iloc[0]\n",
    "    a = words_to_series_weighted(row.get(\"twA\"));  b = words_to_series_weighted(row.get(\"twB\"))\n",
    "    a = clean_token_series(a, stoplist); b = clean_token_series(b, stoplist)\n",
    "    a = a/a.sum() if a.sum()>0 else a\n",
    "    b = b/b.sum() if b.sum()>0 else b\n",
    "    common = list(set(a.index) & set(b.index))\n",
    "    common = sorted(common, key=lambda w: min(a.get(w,0), b.get(w,0)), reverse=True)[:top]\n",
    "    tiltA = (a - b.reindex(a.index, fill_value=0)).sort_values(ascending=False).head(top).index.tolist()\n",
    "    tiltB = (b - a.reindex(a.index, fill_value=0)).sort_values(ascending=False).head(top).index.tolist()\n",
    "    return {\n",
    "        \"topic\": topic,\n",
    "        \"delta\": float(row[\"delta\"]),\n",
    "        sideA+\"_sent\": float(row[\"sentA\"]),\n",
    "        sideB+\"_sent\": float(row[\"sentB\"]),\n",
    "        \"overlap\": common,\n",
    "        sideA+\"_tilt\": tiltA,\n",
    "        sideB+\"_tilt\": tiltB,\n",
    "        \"jaccard_clean\": float(row.get(\"jaccard_clean\", np.nan)),\n",
    "    }\n",
    "\n",
    "def npr_similarity(avg_sentiment, topwords_df, podcasts, baseline=\"NPR\", min_top_words=5):\n",
    "    rows = []\n",
    "    for p in podcasts:\n",
    "        pair = build_pair_contrasts(avg_sentiment, topwords_df, baseline, p, id_col=\"source_name\")\n",
    "        if pair.empty:\n",
    "            continue\n",
    "        stat = global_token_stats(topwords_df, id_col=\"source_name\")\n",
    "        stoplist_names = make_stoplist(stat, min_total_topics=5, min_podcast_share=0.85)\n",
    "        pairC = add_cleaned_overlap(pair, stoplist_names, min_top_words=min_top_words)\n",
    "        corr = np.corrcoef(pairC[\"sentA\"], pairC[\"sentB\"])[0,1] if pairC[\"sentA\"].std()>0 and pairC[\"sentB\"].std()>0 else np.nan\n",
    "        rows.append({\n",
    "            \"podcast\": p,\n",
    "            \"n_topics\": int(len(pairC)),\n",
    "            \"median_abs_delta\": float(pairC[\"delta\"].abs().median()),\n",
    "            \"sent_corr\": float(corr),\n",
    "            \"median_jaccard_clean\": float(pairC[\"jaccard_clean\"].median(skipna=True)),\n",
    "        })\n",
    "    out = pd.DataFrame(rows)\n",
    "    return out.sort_values(by=[\"sent_corr\",\"median_jaccard_clean\",\"median_abs_delta\"], ascending=[False, False, True])\n",
    "\n",
    "def consensus_gap(avg_sentiment, topwords_df, podcasts, baseline=\"NPR\"):\n",
    "    rows = []\n",
    "    for p in podcasts:\n",
    "        pair = build_pair_contrasts(avg_sentiment, topwords_df, baseline, p, id_col=\"source_name\")\n",
    "        rows.append(pair[[\"topic\",\"delta\"]].assign(podcast=p))\n",
    "    allp = pd.concat(rows)\n",
    "    pivot = allp.pivot(index=\"topic\", columns=\"podcast\", values=\"delta\")\n",
    "    sign_share = (pivot.gt(0).sum(axis=1) / pivot.shape[1]).rename(\"share_podcasts_more_pos\")\n",
    "    mean_gap = pivot.mean(axis=1).rename(\"mean_delta_vs_NPR\")\n",
    "    return pd.concat([sign_share, mean_gap], axis=1).sort_values(\"share_podcasts_more_pos\", ascending=False)\n",
    "\n",
    "def map_category(topic):\n",
    "    t = topic.lower()\n",
    "    if \"election\" in t or \"congress\" in t or \"politic\" in t or \"trump\" in t or \"biden\" in t: return \"politics\"\n",
    "    if \"health\" in t or \"care\" in t or \"disease\" in t or \"covid\" in t: return \"health\"\n",
    "    if \"crime\" in t or \"police\" in t or \"court\" in t: return \"justice\"\n",
    "    if \"econom\" in t or \"market\" in t or \"business\" in t or \"income\" in t: return \"economy\"\n",
    "    if \"foreign\" in t or \"war\" in t or \"israel\" in t or \"ukraine\" in t or \"china\" in t: return \"world\"\n",
    "    return \"other\"\n",
    "\n",
    "def topic_view(pair_df, topic, sideA=\"Podcasts\", sideB=\"News\", top=6):\n",
    "    row = pair_df.loc[pair_df[\"topic\"]==topic].iloc[0]\n",
    "    a = words_to_series_weighted(row.get(\"twA\"));  b = words_to_series_weighted(row.get(\"twB\"))\n",
    "    a = clean_token_series(a, stoplist); b = clean_token_series(b, stoplist)\n",
    "    a = a/a.sum() if a.sum()>0 else a\n",
    "    b = b/b.sum() if b.sum()>0 else b\n",
    "    common = list(set(a.index) & set(b.index))\n",
    "    common = sorted(common, key=lambda w: min(a.get(w,0), b.get(w,0)), reverse=True)[:top]\n",
    "    tiltA = (a - b.reindex(a.index, fill_value=0)).sort_values(ascending=False).head(top).index.tolist()\n",
    "    tiltB = (b - a.reindex(b.index, fill_value=0)).sort_values(ascending=False).head(top).index.tolist()\n",
    "    return {\n",
    "        \"topic\": topic,\n",
    "        \"delta\": float(row[\"delta\"]),\n",
    "        sideA+\"_sent\": float(row[\"sentA\"]),\n",
    "        sideB+\"_sent\": float(row[\"sentB\"]),\n",
    "        \"overlap\": common,\n",
    "        sideA+\"_tilt\": tiltA,\n",
    "        sideB+\"_tilt\": tiltB,\n",
    "        \"jaccard_clean\": float(row.get(\"jaccard_clean\", np.nan)),\n",
    "    }\n",
    "\n",
    "def map_topic_to_label(topic):\n",
    "    return topic\n",
    "def consensus_label(topic_row):\n",
    "    return topic_row\n",
    "\n",
    "def find_topics(pair_df, query=\"trump\"):\n",
    "    q = query.lower()\n",
    "    return sorted([t for t in pair_df[\"topic\"].unique() if q in t.lower()])\n",
    "\n",
    "def topics_overview(pair_df, query=\"trump\"):\n",
    "    topics = find_topics(pair_df, query=query)\n",
    "    return [topic_view(pair_df, t) for t in topics]\n",
    "\n",
    "def extremes_by_source_S(S, id_col=\"source_name\", topic_col=\"topic\", sentiment_col=\"avg_sentiment_score\", k=10):\n",
    "    out = {}\n",
    "    for src, g in S.groupby(id_col):\n",
    "        g2 = g[[topic_col, sentiment_col]].dropna()\n",
    "        out[src] = {\n",
    "            \"most_positive\": g2.sort_values(sentiment_col, ascending=False).head(k).values.tolist(),\n",
    "            \"most_negative\": g2.sort_values(sentiment_col, ascending=True).head(k).values.tolist(),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "def topics_by_source(S, id_col=\"source_name\"):\n",
    "    return S.groupby(id_col).size().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
